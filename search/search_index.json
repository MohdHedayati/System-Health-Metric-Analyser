{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"System Health AI","text":""},{"location":"#intelligent-device-diagnostics-platform","title":"Intelligent Device Diagnostics Platform","text":"<p>An end-to-end AI-powered system health monitoring and diagnostics system combining desktop system analytics, cloud-based intelligence, agentic AI, and machine learning.</p> <p>This platform integrates local telemetry collection, secure cloud synchronization, retrieval-augmented reasoning, and autonomous AI agents to deliver actionable insights into device health, performance, and stability.</p>"},{"location":"#project-scope","title":"\ud83d\ude80 Project Scope","text":"<p>This project is a multi-layered intelligent system consisting of:</p> <ul> <li> <p>\ud83d\udda5 Local Desktop Application (PyQt5) \u00a0   Secure system monitoring, hardware telemetry collection, ML inference, and report generation.</p> </li> <li> <p>\ud83c\udf10 Cloud Web Application (Streamlit) \u00a0   AI-powered dashboards, chatbot-based diagnostics, report visualization, and analytics.</p> </li> <li> <p>\ud83e\udde0 AI + Agentic Systems \u00a0   Generative LLMs, Retrieval-Augmented Generation (RAG), and agent-based reasoning for intelligent diagnostics.</p> </li> <li> <p>\ud83d\udcca Machine Learning Pipeline \u00a0   Neural networks and encoders for system health trend analysis and predictive insights.</p> </li> <li> <p>\u2601 Backend Infrastructure \u00a0   Supabase-powered cloud backend with full CRUD support and authentication mapping.</p> </li> </ul>"},{"location":"#core-capabilities","title":"\ud83e\udde0 Core Capabilities","text":"<ul> <li>Intelligent system health diagnostics \u00a0</li> <li>Real-time and historical telemetry analysis \u00a0</li> <li>AI-powered chatbot for troubleshooting \u00a0</li> <li>Secure cross-platform authentication \u00a0</li> <li>Cloud-based analytics dashboard \u00a0</li> <li>Autonomous agent-driven insights \u00a0</li> </ul>"},{"location":"#technology-stack","title":"\ud83c\udfd7 Technology Stack","text":"Layer Technologies Desktop App PyQt5, psutil Web App Streamlit AI LLMs, Agentic AI, RAG ML Encoders, Neural Networks Backend Supabase Auth Google OAuth Vector DB ChromaDB"},{"location":"#project-objective","title":"\ud83c\udfaf Project Objective","text":"<p>To move beyond traditional hardware monitoring tools by providing intelligent diagnostics instead of raw metrics, enabling users to:</p> <ul> <li>Understand system behavior</li> <li>Detect performance anomalies</li> <li>Anticipate potential failures</li> <li>Make informed optimization decisions</li> </ul>"},{"location":"#getting-started","title":"\ud83d\udccc Getting Started","text":"<p>Use the navigation panel to explore:</p> <ul> <li>System Architecture \u00a0</li> <li>Desktop Application \u00a0</li> <li>Web Dashboard \u00a0</li> <li>AI &amp; ML Systems \u00a0</li> <li>Backend Infrastructure \u00a0</li> <li>Development Timeline \u00a0</li> </ul> <p>System Health AI \u2014 Turning raw system data into intelligent insights.</p>"},{"location":"acknowledgements/","title":"Acknowledgements","text":"<p>This project was made possible through the collaborative efforts, dedication, and continuous learning of a passionate development team, along with the valuable guidance of our mentors.</p>"},{"location":"acknowledgements/#development-team","title":"\ud83d\udc68\u200d\ud83d\udcbb Development Team","text":"<p>This system was designed, engineered, and implemented by:</p> <ul> <li>Mohammed Mehdi Hedayati</li> <li>Awwab Wadekar</li> <li>Nathan Dsouza</li> <li>Taha Valiji</li> </ul> <p>Each member contributed significantly across system design, backend development, machine learning, AI integration, frontend development, and architectural planning. The project reflects the collective effort, creativity, and technical depth of the entire team.</p>"},{"location":"acknowledgements/#mentorship-guidance","title":"\ud83c\udf93 Mentorship &amp; Guidance","text":"<p>We extend our sincere gratitude to our mentors:</p> <ul> <li>Amal Verma</li> <li>Prathamesh Sankhe</li> </ul> <p>Their continuous support, technical guidance, constructive feedback, and encouragement played a crucial role in shaping the direction, execution, and refinement of this project.</p>"},{"location":"acknowledgements/#special-thanks","title":"\ud83d\ude4f Special Thanks","text":"<p>We also acknowledge the broader academic and developer communities whose open-source tools, research publications, and frameworks helped accelerate development and innovation throughout this project.</p> <p>This project stands as a testament to collaborative learning, technical exploration, and a shared passion for building intelligent systems.</p>"},{"location":"architecture/","title":"System Architecture","text":"<p>This project implements a distributed intelligent diagnostics platform combining system-level monitoring, machine learning, agentic AI, retrieval-augmented generation (RAG), and full-stack web technologies.</p> <p>The system is architected into two tightly coupled applications:</p> <ul> <li>Desktop System Agent (PyQt5) \u2013 responsible for hardware-level data collection, ML inference, and secure reporting.</li> <li>Web Dashboard (Streamlit) \u2013 responsible for visualization, AI-powered diagnostics, and cloud-based analytics.</li> </ul>"},{"location":"architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TD\n    %% Styling\n    classDef client fill:#e1f5fe,stroke:#01579b,stroke-width:2px;\n    classDef cloud fill:#f3e5f5,stroke:#4a148c,stroke-width:2px;\n    classDef db fill:#fff3e0,stroke:#e65100,stroke-width:2px;\n    classDef ml fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px;\n\n    subgraph Client_Side [User Local Environment]\n        direction TB\n        User((User))\n\n        subgraph Desktop_App [PyQt5 Desktop Agent]\n            Auth_Local[Google OAuth]\n            Metrics[psutil Metrics Engine]\n            Report_Gen[Report Generator]\n\n            subgraph ML_Engine [Local Intelligence]\n                Neural_Net[Neural Networks]\n                Anomaly[Anomaly Detection]\n            end\n        end\n    end\n\n    subgraph Cloud_Side [Cloud Infrastructure]\n        direction TB\n\n        subgraph Backend [Supabase Backend]\n            Auth_DB[(Auth &amp; Users)]\n            Metrics_DB[(Metrics Storage)]\n            Vector_DB[(Vector Store)]\n        end\n\n        subgraph Web_Platform [Streamlit Web Dashboard]\n            Dashboard[Visual Dashboard]\n            Chat_UI[AI Chat Interface]\n\n            subgraph AI_Core [Agentic AI Engine]\n                RAG[RAG Pipeline]\n                LLM[LLM Reasoning]\n                Agents[Autonomous Agents]\n            end\n        end\n    end\n\n    %% Connections\n    User --&gt;|Starts| Desktop_App\n    User --&gt;|Visits| Web_Platform\n\n    %% Desktop Flows\n    Metrics --&gt;|Raw Data| Neural_Net\n    Neural_Net --&gt;|Inference| Anomaly\n    Anomaly --&gt;|Insights| Report_Gen\n    Metrics --&gt;|Stats| Report_Gen\n    Report_Gen --&gt;|Secure Upload| Metrics_DB\n\n    %% Cloud Flows\n    Web_Platform --&gt;|Fetch Data| Metrics_DB\n    Chat_UI --&gt;|Query| AI_Core\n    AI_Core --&gt;|Retrieve Context| Vector_DB\n    AI_Core --&gt;|Response| Chat_UI\n\n    %% Styles\n    class Desktop_App,Web_Platform client;\n    class Backend,AI_Core cloud;\n    class Auth_DB,Metrics_DB,Vector_DB db;\n    class ML_Engine,Neural_Net,Anomaly ml;</code></pre>"},{"location":"architecture/#architectural-principles","title":"Architectural Principles","text":""},{"location":"architecture/#1-separation-of-concerns","title":"1. Separation of Concerns","text":"<p>The system separates responsibilities between:</p> <ul> <li>Local System Intelligence</li> <li>Cloud-based AI Reasoning</li> <li>Persistent Storage &amp; Authentication</li> </ul> <p>This ensures: - Security - Scalability - Platform independence - Modularity</p>"},{"location":"architecture/#2-dual-application-design","title":"2. Dual Application Design","text":""},{"location":"architecture/#desktop-system-agent-pyqt5","title":"Desktop System Agent (PyQt5)","text":"<p>The desktop agent is responsible for hardware-sensitive operations, which are not possible inside browser-based applications due to sandboxing and security constraints.</p> <p>Responsibilities:</p> <ul> <li>Secure Google OAuth authentication</li> <li>Real-time system metric collection using <code>psutil</code></li> <li>Local ML inference using trained neural networks</li> <li>Structured health report generation</li> <li>Secure upload of reports to Supabase</li> </ul> <p>The desktop client acts as a trusted system sensor and intelligence node.</p>"},{"location":"architecture/#web-dashboard-streamlit","title":"Web Dashboard (Streamlit)","text":"<p>The web application provides:</p> <ul> <li>Cloud-based visualization</li> <li>Historical report analysis</li> <li>AI-powered diagnostics chatbot</li> <li>Interactive dashboards</li> </ul> <p>Responsibilities:</p> <ul> <li>Secure OAuth login</li> <li>Supabase data access</li> <li>RAG pipeline execution</li> <li>LLM-powered conversational diagnostics</li> <li>UX-focused interface design</li> </ul>"},{"location":"architecture/#data-flow-pipeline","title":"Data Flow Pipeline","text":""},{"location":"architecture/#step-1-metric-collection-desktop","title":"Step 1 \u2014 Metric Collection (Desktop)","text":"<p>System metrics are collected using <code>psutil</code>, including:</p> <ul> <li>CPU utilization and frequency</li> <li>Memory usage</li> <li>Disk usage and I/O</li> <li>Network activity</li> <li>Temperature sensors (when available)</li> <li>Active process statistics</li> </ul>"},{"location":"architecture/#step-2-local-ml-inference","title":"Step 2 \u2014 Local ML Inference","text":"<p>Collected metrics are passed into trained neural classifiers that perform:</p> <ul> <li>Real-time anomaly detection</li> <li>Subsystem classification:</li> <li>CPU + Memory</li> <li>Disk</li> <li>Network</li> <li>Temperature</li> </ul> <p>This enables immediate local detection of suspicious system behavior.</p>"},{"location":"architecture/#step-3-agentic-intelligence-engine","title":"Step 3 \u2014 Agentic Intelligence Engine","text":"<p>ML outputs are combined with:</p> <ul> <li>Statistical heuristics</li> <li>Domain-driven threat signatures</li> <li>Multi-dimensional risk scoring</li> </ul> <p>The system performs:</p> <ul> <li>Cryptojacking detection</li> <li>Data exfiltration detection</li> <li>Thermal abuse analysis</li> <li>Sustained workload profiling</li> <li>Root cause process attribution</li> </ul> <p>This hybrid architecture combines:</p> <p>Neural Intelligence + Rule-based Expert Systems</p>"},{"location":"architecture/#step-4-structured-report-generation","title":"Step 4 \u2014 Structured Report Generation","text":"<p>A comprehensive health report is generated containing:</p> <ul> <li>Global threat index</li> <li>Subsystem anomaly breakdown</li> <li>Root cause processes</li> <li>Aggregate window analysis</li> <li>Predictive system forecasts</li> </ul> <p>This structured format enables seamless cloud analysis and AI interpretation.</p>"},{"location":"architecture/#step-5-secure-cloud-synchronization","title":"Step 5 \u2014 Secure Cloud Synchronization","text":"<p>Reports are uploaded to Supabase using secure API access.</p> <p>Supabase provides:</p> <ul> <li>Authentication mapping</li> <li>Report storage</li> <li>Chat history storage</li> <li>Full CRUD support</li> </ul>"},{"location":"architecture/#step-6-ai-reasoning-rag","title":"Step 6 \u2014 AI Reasoning &amp; RAG","text":"<p>The Streamlit web application integrates:</p> <ul> <li>Vector database powered RAG pipeline</li> <li>OS-specific contextual knowledge bases</li> <li>Generative LLM chatbot</li> </ul> <p>This allows:</p> <ul> <li>Context-aware diagnostics</li> <li>Explainable system health insights</li> <li>OS-tailored optimization guidance</li> </ul>"},{"location":"architecture/#ai-system-design","title":"AI System Design","text":""},{"location":"architecture/#generative-ai","title":"Generative AI","text":"<ul> <li>LLM-based conversational diagnostics</li> <li>System health explanation</li> <li>Actionable optimization guidance</li> </ul>"},{"location":"architecture/#agentic-ai","title":"Agentic AI","text":"<ul> <li>Autonomous threat reasoning</li> <li>Root cause inference</li> <li>Predictive trend modeling</li> <li>Decision fusion across subsystems</li> </ul>"},{"location":"architecture/#machine-learning-layer","title":"Machine Learning Layer","text":"<ul> <li>Neural classifiers for:</li> <li>CPU + memory anomalies</li> <li>Disk anomalies</li> <li>Network anomalies</li> <li>Thermal anomalies</li> <li>Encoders for feature extraction</li> <li>Supervised training on synthetic + real workload profiles</li> </ul>"},{"location":"architecture/#why-this-architecture-matters","title":"Why This Architecture Matters","text":"<p>This system replicates real-world observability and cybersecurity architectures used in:</p> <ul> <li>Enterprise monitoring platforms</li> <li>Endpoint detection systems (EDR)</li> <li>Cloud telemetry analytics</li> <li>Predictive maintenance systems</li> </ul> <p>By integrating:</p> <ul> <li>Desktop-level sensing</li> <li>Cloud-scale AI reasoning</li> <li>Hybrid intelligence models</li> </ul> <p>the project achieves industrial-grade system diagnostics capabilities.</p>"},{"location":"architecture/#system-capabilities-summary","title":"System Capabilities Summary","text":"Layer Capability Desktop Real-time monitoring + ML inference Backend Secure storage + authentication AI Generative + Agentic + RAG Web Visualization + conversational diagnostics ML Trend modeling + anomaly classification <p>This architecture enables scalable, secure, and intelligent system diagnostics across platforms.</p>"},{"location":"future/","title":"Future Scope &amp; Roadmap","text":"<p>System Health AI is designed as a scalable, extensible intelligent diagnostics platform. The current implementation forms a strong foundation, while multiple advanced capabilities are planned to expand its intelligence, automation, and enterprise readiness.</p>"},{"location":"future/#short-term-enhancements-next-phase","title":"\ud83d\ude80 Short-Term Enhancements (Next Phase)","text":""},{"location":"future/#1-real-time-streaming-analytics","title":"1. Real-Time Streaming Analytics","text":"<ul> <li>Live metric streaming using WebSockets or Kafka</li> <li>Millisecond-level anomaly detection</li> <li>Live alert generation and push notifications</li> </ul>"},{"location":"future/#2-advanced-ml-models","title":"2. Advanced ML Models","text":"<ul> <li>Transformer-based temporal models for:</li> <li>System behavior prediction</li> <li>Failure forecasting</li> <li>Autoencoder-based unsupervised anomaly detection</li> <li>Federated learning across devices for privacy-preserving training</li> </ul>"},{"location":"future/#3-adaptive-agentic-ai","title":"3. Adaptive Agentic AI","text":"<ul> <li>Multi-agent reasoning pipelines</li> <li>Self-healing workflows:</li> <li>Automatic service restart</li> <li>Dynamic resource throttling</li> <li>Goal-based system optimization agents</li> </ul>"},{"location":"future/#advanced-ai-capabilities","title":"\ud83e\udde0 Advanced AI Capabilities","text":""},{"location":"future/#1-autonomous-diagnostics-engine","title":"1. Autonomous Diagnostics Engine","text":"<ul> <li>Root cause analysis agents</li> <li>Multi-step incident resolution planning</li> <li>System optimization suggestions</li> </ul>"},{"location":"future/#2-continuous-learning-loop","title":"2. Continuous Learning Loop","text":"<ul> <li>Online learning from real-world telemetry</li> <li>Model refinement using feedback loops</li> <li>Adaptive threshold tuning</li> </ul>"},{"location":"future/#3-explainable-ai-xai","title":"3. Explainable AI (XAI)","text":"<ul> <li>Model interpretability dashboards</li> <li>Human-readable diagnostic explanations</li> <li>Feature attribution visualizations</li> </ul>"},{"location":"future/#security-enterprise-expansion","title":"\ud83d\udd10 Security &amp; Enterprise Expansion","text":""},{"location":"future/#1-zero-trust-architecture","title":"1. Zero Trust Architecture","text":"<ul> <li>Continuous device identity verification</li> <li>Adaptive risk-based access control</li> <li>Endpoint trust scoring</li> </ul>"},{"location":"future/#2-threat-intelligence-integration","title":"2. Threat Intelligence Integration","text":"<ul> <li>Real-time malware signature ingestion</li> <li>SOC (Security Operations Center) integrations</li> <li>SIEM compatibility</li> </ul>"},{"location":"future/#3-compliance-support","title":"3. Compliance Support","text":"<ul> <li>GDPR / HIPAA / SOC2 compliance modules</li> <li>Enterprise audit logging</li> <li>Policy-driven data governance</li> </ul>"},{"location":"future/#cloud-native-scaling","title":"\u2601 Cloud-Native Scaling","text":""},{"location":"future/#1-kubernetes-deployment","title":"1. Kubernetes Deployment","text":"<ul> <li>Containerized microservices</li> <li>Horizontal auto-scaling</li> <li>High-availability clusters</li> </ul>"},{"location":"future/#2-edge-computing","title":"2. Edge Computing","text":"<ul> <li>On-device AI inference</li> <li>Low-latency decision making</li> <li>Offline anomaly detection</li> </ul>"},{"location":"future/#platform-expansion","title":"\ud83c\udf10 Platform Expansion","text":""},{"location":"future/#1-mobile-application","title":"1. Mobile Application","text":"<ul> <li>Android &amp; iOS apps</li> <li>Push alerts</li> <li>Remote monitoring</li> </ul>"},{"location":"future/#2-cross-platform-desktop-clients","title":"2. Cross-Platform Desktop Clients","text":"<ul> <li>Native macOS &amp; Linux builds</li> <li>Unified monitoring dashboard</li> </ul>"},{"location":"future/#research-oriented-extensions","title":"\ud83e\uddea Research-Oriented Extensions","text":""},{"location":"future/#1-predictive-failure-modeling","title":"1. Predictive Failure Modeling","text":"<ul> <li>Component lifetime prediction</li> <li>Hardware degradation modeling</li> </ul>"},{"location":"future/#2-reinforcement-learning","title":"2. Reinforcement Learning","text":"<ul> <li>System performance optimization agents</li> <li>Resource allocation learning loops</li> </ul>"},{"location":"future/#3-digital-twin-simulation","title":"3. Digital Twin Simulation","text":"<ul> <li>Virtual replica of system states</li> <li>Failure simulation and stress testing</li> </ul>"},{"location":"future/#long-term-vision","title":"\ud83d\udee3 Long-Term Vision","text":"<p>To evolve System Health AI into a fully autonomous system intelligence platform, capable of:</p> <ul> <li>Predicting failures before they occur</li> <li>Self-healing operating systems</li> <li>Autonomous system tuning</li> <li>Enterprise-grade security monitoring</li> <li>Cross-platform observability</li> </ul> <p>The long-term objective is to build a self-aware, adaptive, and autonomous digital nervous system for computing devices.</p>"},{"location":"overview/","title":"Project Overview","text":"<p>System Health AI is a full-stack intelligent diagnostics platform designed to analyze, understand, and predict system behavior using machine learning, generative AI, and autonomous agents.</p> <p>Traditional system monitoring tools expose raw performance metrics. While useful, these metrics often fail to explain why a problem occurs, how serious it is, and what action should be taken.</p> <p>This project bridges that gap by combining local telemetry analysis, cloud-based intelligence, and AI-driven reasoning to deliver actionable system insights instead of raw numbers.</p>"},{"location":"overview/#what-this-system-does","title":"\ud83d\udd0d What This System Does","text":"<p>The platform continuously collects and analyzes system telemetry such as:</p> <ul> <li>CPU utilization and frequency  </li> <li>Memory usage patterns  </li> <li>Disk I/O behavior  </li> <li>Network traffic characteristics  </li> <li>Thermal sensor data  </li> <li>Active process behavior  </li> </ul> <p>These metrics are processed using:</p> <ul> <li>Machine learning models for anomaly detection  </li> <li>Neural encoders for behavior representation  </li> <li>LLM-based reasoning for diagnostics  </li> <li>Agentic AI pipelines for autonomous decision-making  </li> </ul>"},{"location":"overview/#intelligent-diagnostics-pipeline","title":"\ud83e\udde0 Intelligent Diagnostics Pipeline","text":"<p>The system follows a multi-stage diagnostic pipeline:</p> <ol> <li> <p>Local Telemetry Collection    System metrics are collected securely using <code>psutil</code> and platform-specific APIs.</p> </li> <li> <p>ML-Based Pattern Detection    Neural networks detect short-term anomalies and long-term behavior deviations.</p> </li> <li> <p>Cloud Synchronization    Encrypted reports are securely uploaded to the backend for long-term analytics.</p> </li> <li> <p>RAG-Powered Reasoning    LLMs analyze reports using retrieval-augmented context specific to each operating system.</p> </li> <li> <p>Agentic Intelligence    Autonomous agents generate system insights, mitigation steps, and performance forecasts.</p> </li> </ol>"},{"location":"overview/#multi-platform-design","title":"\ud83c\udf0d Multi-Platform Design","text":"<p>The system is designed for cross-platform compatibility:</p> <ul> <li>Windows  </li> <li>Ubuntu Linux  </li> <li>Arch Linux  </li> </ul> <p>Each platform includes OS-specific telemetry collection, optimization rules, and RAG knowledge bases.</p>"},{"location":"overview/#modular-architecture","title":"\ud83e\udde9 Modular Architecture","text":"<p>The platform is split into modular layers:</p> <ul> <li>Desktop Client \u2014 Data collection, ML inference, encryption, reporting  </li> <li>Backend Services \u2014 Authentication, database, vector storage, API endpoints  </li> <li>Web Dashboard \u2014 Visualization, chatbot interface, analytics  </li> <li>AI Engine \u2014 LLM pipelines, RAG, agent workflows  </li> <li>ML Engine \u2014 Neural classifiers, encoders, anomaly detectors  </li> </ul> <p>This modular design allows independent scaling, development, and testing.</p>"},{"location":"overview/#key-objectives","title":"\ud83c\udfaf Key Objectives","text":"<ul> <li>Deliver intelligent diagnostics instead of raw metrics</li> <li>Detect early-stage system degradation</li> <li>Predict future performance bottlenecks</li> <li>Provide explainable AI-driven insights</li> <li>Enable secure cross-device monitoring</li> </ul>"},{"location":"overview/#target-use-cases","title":"\ud83c\udfc6 Target Use Cases","text":"<ul> <li>Advanced system monitoring  </li> <li>Cybersecurity anomaly detection  </li> <li>Hardware fault prediction  </li> <li>Performance optimization  </li> <li>System behavior analytics  </li> </ul>"},{"location":"overview/#future-roadmap","title":"\ud83d\udd2e Future Roadmap","text":"<ul> <li>Federated learning across devices  </li> <li>Predictive failure modeling  </li> <li>Auto-remediation agents  </li> <li>Enterprise fleet monitoring  </li> <li>Mobile companion application  </li> </ul> <p>System Health AI transforms device monitoring into an intelligent decision-making system.</p>"},{"location":"security/","title":"Security &amp; Privacy","text":"<p>Building a system that monitors hardware and process activity requires a high level of trust. We approach security with a \"Privacy by Design\" philosophy, ensuring that user data is encrypted, isolated, and strictly scoped.</p> <p>This document outlines the security measures implemented across the Desktop Agent, Cloud Backend, and AI Pipelines.</p>"},{"location":"security/#data-collection-policy","title":"\ud83d\udee1\ufe0f Data Collection Policy","text":"<p>We believe in Data Minimization. We collect only the telemetry necessary to diagnose system health.</p>"},{"location":"security/#what-we-collect","title":"\u2705 What We Collect","text":"<ul> <li>Performance Metrics: CPU load, RAM usage, Temperatures, Fan speeds.</li> <li>Hardware Signatures: CPU model name, Total Memory, Disk capacity (to create a \"baseline\" for the AI).</li> <li>Process Statistics: Resource consumption of running applications (e.g., \"chrome.exe uses 2GB RAM\").</li> <li>Anomalies: Events flagged by the ML engine (e.g., \"High Thermal Event\").</li> </ul>"},{"location":"security/#what-we-do-not-collect","title":"\u274c What We Do NOT Collect","text":"<ul> <li>Keystrokes: The agent has no keylogging capabilities.</li> <li>File Contents: We scan disk usage (I/O rates), but we never read or upload the contents of your documents.</li> <li>Browser History: We monitor network traffic volume, not the websites you visit.</li> <li>Screen Content: The application never takes screenshots.</li> </ul>"},{"location":"security/#architecture-security-layers","title":"\ud83d\udd10 Architecture Security Layers","text":"<p>We employ a \"Defense in Depth\" strategy, securing the data at every stage of its lifecycle.</p>"},{"location":"security/#1-security-in-transit-encryption","title":"1. Security in Transit (Encryption)","text":"<p>All communication between the Desktop Agent, the Web Dashboard, and the Cloud Backend occurs over HTTPS (TLS 1.2/1.3). * Certificate Pinning: The desktop agent validates SSL certificates to prevent Man-in-the-Middle (MITM) attacks. * No Plaintext: API keys and Tokens are never sent in the URL parameters; they are passed strictly in HTTP Headers.</p>"},{"location":"security/#2-security-at-rest-database","title":"2. Security at Rest (Database)","text":"<p>Data stored in Supabase is encrypted using AES-256 (Advanced Encryption Standard). * Row Level Security (RLS): As detailed in the backend documentation, the database engine itself enforces strict isolation. A user with ID <code>A</code> cannot query the rows belonging to User <code>B</code>, even if they manage to send a custom SQL query.</p>"},{"location":"security/#3-desktop-security","title":"3. Desktop Security","text":"<ul> <li>Token Storage: OAuth Refresh Tokens are stored using the operating system's native secure vault (Windows Credential Manager / Linux Keyring) via the <code>keyring</code> Python library. They are never saved in plain text files.</li> <li>Report Signing: Before uploading, the desktop agent signs the JSON payload with a hash. This ensures that the data received by the cloud hasn't been tampered with by malware on the local machine.</li> </ul>"},{"location":"security/#ai-privacy","title":"\ud83e\udd16 AI &amp; Privacy","text":"<p>Integrating Generative AI introduces new privacy questions. Here is how we handle them:</p>"},{"location":"security/#no-public-training","title":"No Public Training","text":"<p>Your specific system logs are NOT used to train public models (like generic ChatGPT). * Context Injection: We use RAG (Retrieval-Augmented Generation). Your data is sent to the LLM only during your active chat session to provide context, and is then discarded from the model's short-term memory. * Anonymized Aggregation: The internal Neural Networks (for anomaly detection) are trained on aggregated, anonymized datasets where all user identifiers and specific file paths have been stripped out.</p>"},{"location":"security/#user-control-gdprccpa","title":"\ud83d\udc64 User Control (GDPR/CCPA)","text":"<p>Users retain full ownership of their data. Through the Web Dashboard settings, users can execute the following actions:</p> <ol> <li>Purge History: Delete all historical telemetry logs immediately.</li> <li>Unlink Device: Revoke access for a specific desktop agent.</li> <li>Download Data: Export all stored metrics in JSON/CSV format.</li> </ol>"},{"location":"security/#vulnerability-reporting","title":"\ud83d\udc1b Vulnerability Reporting","text":"<p>Security is an ongoing process. If you discover a vulnerability in the Desktop Agent or API, please open a strictly confidential issue on our GitHub repository using the \"Security Advisory\" tag.</p>"},{"location":"ai/agentic/","title":"Agentic AI Engine","text":"<p>The Agentic AI Engine is the autonomous reasoning core of the platform. While the Generative AI component handles language and communication, the Agentic Engine is responsible for taking action and solving problems.</p> <p>It transforms the system from a passive monitoring tool into an active \"Virtual System Administrator\" that can investigate root causes on its own.</p>"},{"location":"ai/agentic/#the-react-paradigm","title":"\ud83e\udde0 The ReAct Paradigm","text":"<p>The engine is built upon the ReAct (Reason + Act) pattern. Instead of immediately trying to guess an answer, the AI enters a reasoning loop:</p> <ol> <li>Thought: The agent analyzes the user's request to understand the intent.</li> <li>Action: It selects a specific tool from its toolkit to gather evidence.</li> <li>Observation: It reads the output of that tool (e.g., a database query result).</li> <li>Synthesis: It combines the observation with its internal knowledge to form a conclusion.</li> </ol> <p>If the first observation isn't enough, it repeats the loop until it solves the problem.</p>"},{"location":"ai/agentic/#the-agents-toolbelt","title":"\ud83d\udee0 The Agent's Toolbelt","text":"<p>For an AI to be \"Agentic,\" it must have hands. In this architecture, \"hands\" are Python functions (Tools) that the LLM is allowed to execute.</p> Tool Name Purpose Example Trigger <code>MetricRetriever</code> Queries Supabase for raw telemetry data (CPU, RAM, Disk). \"Check if the CPU was high 10 mins ago.\" <code>VectorSearch</code> Queries the RAG system for documentation and fixes. \"How do I fix error 0x8004?\" <code>LogInspector</code> (Planned) Fetches specific error logs from the OS. \"Check the system event logs.\" <code>AnomalyChecker</code> Retrieves flags set by the Neural Network. \"Did the ML model detect anything?\""},{"location":"ai/agentic/#example-investigation-workflow","title":"\ud83d\udd75\ufe0f Example Investigation Workflow","text":"<p>User Query: \"Why is my internet so slow?\"</p> <p>Step 1: Thought</p> <p>\"The user is reporting network issues. I need to check if the network interface is saturated or if there are errors.\"</p> <p>Step 2: Action</p> <p>Calls <code>MetricRetriever.get_network_stats(duration=\"30m\")</code></p> <p>Step 3: Observation</p> <p>Result: { \"download_speed\": \"0.5 MB/s\", \"packet_drop_rate\": \"15%\" }</p> <p>Step 4: Thought</p> <p>\"Download speed is low, but the packet drop rate is very high (15%). This usually indicates a bad Wi-Fi signal or a faulty cable, not just congestion.\"</p> <p>Step 5: Final Response</p> <p>\"I checked your network stats for the last 30 minutes. While your speed is low (0.5 MB/s), the main issue is a 15% packet drop rate. This suggests a physical hardware issue (like a bad cable) or poor Wi-Fi reception, rather than just a slow ISP connection.\"</p>"},{"location":"ai/agentic/#safety-guardrails","title":"\ud83d\udee1 Safety &amp; Guardrails","text":"<p>To prevent the agent from hallucinating or executing dangerous commands, strict guardrails are enforced:</p> <ul> <li>Read-Only Access: Currently, the agent can read system data but cannot execute system commands (like <code>rm -rf</code> or <code>shutdown</code>).</li> <li>Scoped Queries: Database queries are strictly scoped to the authenticated user's ID.</li> <li>Confidence Thresholds: If the agent is unsure, it is programmed to ask for clarification rather than guessing.</li> </ul>"},{"location":"ai/agentic/#why-this-matters","title":"\ud83d\ude80 Why This Matters","text":"<p>Standard LLMs (like ChatGPT) cannot see your computer's screen or internal state. They can only offer generic advice.</p> <p>By giving the LLM tools to look at your actual <code>psutil</code> data, the Agentic Engine bridges the gap between general knowledge and specific reality, providing diagnostics that are actually relevant to your machine's current situation.</p>"},{"location":"ai/generative/","title":"Generative AI Pipeline","text":"<p>While the Agentic Engine handles reasoning and logic, the Generative AI Pipeline is responsible for communication. It acts as the \"Translator\" between the cold, binary world of system logs and the natural language needed by the user.</p> <p>Its primary goal is to turn complex JSON objects (like <code>cpu_load: 0.82</code>) into meaningful explanations (e.g., \"Your processor is under heavy load, likely due to a background update\").</p>"},{"location":"ai/generative/#the-role-of-the-llm","title":"\ud83d\udde3 The Role of the LLM","text":"<p>In this architecture, the Large Language Model (LLM) serves three specific functions:</p> <ol> <li>Summarization: Taking 500 rows of system logs and condensing them into a single, digestible paragraph.</li> <li>Translation: converting technical error codes (e.g., <code>0x80040154</code>) into plain English descriptions.</li> <li>Tone Calibration: Adjusting the complexity of the explanation based on who is asking (e.g., explaining a crash simply to a casual user vs. providing technical details to a developer).</li> </ol>"},{"location":"ai/generative/#prompt-engineering-strategy","title":"\ud83d\udcdd Prompt Engineering Strategy","text":"<p>To ensure the AI behaves like a reliable Systems Administrator rather than a creative writer, we utilize strict System Prompts.</p> <p>These prompts \"program\" the behavior of the model before the user ever asks a question.</p>"},{"location":"ai/generative/#the-sysadmin-persona","title":"The \"SysAdmin\" Persona","text":"<p>A typical System Message injected into the context looks like this:</p> <p>\"You are an expert Systems Reliability Engineer. You are analyzing a Windows 11 machine.</p> <p>Rules: 1. Be concise and professional. 2. Base your answers ONLY on the provided telemetry context. 3. If the data is missing, admit it. Do not hallucinate metrics. 4. When suggesting fixes, prioritize non-destructive actions first.\"</p> <p>By anchoring the model with these rules, we significantly reduce the chance of \"hallucinations\" (the AI making up facts).</p>"},{"location":"ai/generative/#context-management","title":"\ud83d\udcc4 Context Management","text":"<p>Generative AI models have a Context Window (a limit on how much text they can read at once). Sending a full day's worth of raw logs would overflow this limit immediately.</p> <p>To handle this, the pipeline implements Intelligent Context pruning:</p> <ol> <li>JSON Minimization: Removing redundant keys from the telemetry data to save tokens.</li> <li>Time-Slicing: Only feeding the model data relevant to the user's query time range (e.g., the exact 5-minute window where the crash happened).</li> <li>Key-Event Filtering: Prioritizing \"Critical\" and \"Warning\" logs over \"Info\" logs.</li> </ol>"},{"location":"ai/generative/#model-integration","title":"\ud83d\udd17 Model Integration","text":"<p>The platform is designed to be model-agnostic, connecting via standard API protocols.</p> <ul> <li>Primary Logic: We use high-intelligence models (like GPT-4 or Claude 3.5 Sonnet) for complex reasoning tasks.</li> <li>Fast Response: We use lightweight, fast models (like GPT-4o-mini or Gemini Flash) for quick summaries and chat interactions to keep the dashboard snappy.</li> </ul>"},{"location":"ai/generative/#generative-vs-agentic-vs-ml","title":"\u2696\ufe0f Generative vs. Agentic vs. ML","text":"<p>It is important to distinguish the three AI pillars in this project:</p> Component Role Example Generative AI The Voice. Writes the text and explains the data. \"Your CPU is hot because Chrome is open.\" Agentic AI The Brain. Decides which tools to use. Decides to fetch CPU data from the DB. Machine Learning The Eyes. Detects patterns in raw numbers. Flags a temperature of 95\u00b0C as an anomaly."},{"location":"ai/ml/","title":"ML Encoders &amp; Neural Networks","text":"<p>While Generative AI allows the system to speak, Machine Learning (ML) allows the system to feel.</p> <p>The ML Encoders and Neural Networks form the subconscious monitoring layer of the platform. They run silently in the background, processing thousands of data points per second to answer two fundamental questions: 1.  \"Is the system behaving normally right now?\" (Anomaly Detection) 2.  \"Is the system about to crash?\" (Predictive Maintenance)</p>"},{"location":"ai/ml/#the-limitations-of-standard-monitoring","title":"\ud83c\udfd7 The Limitations of Standard Monitoring","text":"<p>Traditional monitoring tools rely on Static Thresholds: * If CPU &gt; 90%, send alert. * If Temp &gt; 80\u00b0C, send alert.</p> <p>The Problem: These rules are too rigid. * False Positive: A video render should use 100% CPU. That is not an anomaly; it is work. * False Negative: A cryptojacking virus might throttle itself to 50% CPU to stay hidden. A static threshold of 90% will never catch it.</p> <p>The Solution: Neural Networks that learn Contextual Normality.</p>"},{"location":"ai/ml/#model-architecture-the-autoencoder","title":"\ud83e\udde0 Model Architecture: The Autoencoder","text":"<p>The core of our anomaly detection engine is an Autoencoder Neural Network.</p>"},{"location":"ai/ml/#how-it-works","title":"How It Works","text":"<p>An Autoencoder is an unsupervised learning model that learns to copy its input to its output.</p> <ol> <li>Encoder: Compresses the complex system state (CPU, RAM, Disk, Net) into a small \"Latent Vector\" (a summary).</li> <li>Bottleneck: A constrained layer that forces the network to learn only the most important features.</li> <li>Decoder: Attempts to reconstruct the original input from the summary.</li> </ol> <p>[Figure: Diagram showing Input Data flowing through Encoder to Bottleneck, then expanding via Decoder to Output]</p>"},{"location":"ai/ml/#the-anomaly-detection-logic","title":"The Anomaly Detection Logic","text":"<ul> <li>We train the model only on healthy data. It learns exactly what \"Normal\" looks like.</li> <li>When we feed it a Virus or Crash signature, it fails to reconstruct it accurately because it has never seen it before.</li> <li>Reconstruction Error: The difference between Input and Output.<ul> <li>Low Error = Normal Behavior.</li> <li>High Error = Anomaly Detected.</li> </ul> </li> </ul>"},{"location":"ai/ml/#time-series-forecasting-lstm","title":"\ud83d\udcc9 Time-Series Forecasting (LSTM)","text":"<p>While Autoencoders detect current problems, Long Short-Term Memory (LSTM) networks predict future problems.</p> <p>System crashes rarely happen instantly; they usually leave a trail of clues: * Minute 1: Memory usage creeps up 1%. * Minute 5: Disk Swap usage increases. * Minute 10: CPU temperature rises slightly. * Minute 15: Crash.</p> <p>We employ LSTMs to analyze the temporal sequence of data. By remembering the past 60 seconds of metrics, the model can predict the probability of a system freeze occurring in the next 5 minutes.</p>"},{"location":"ai/ml/#feature-engineering-encoders","title":"\ud83d\udd22 Feature Engineering &amp; Encoders","text":"<p>Raw numbers are not enough. We preprocess data into Features to make them understandable for the Neural Network.</p> Raw Metric Engineered Feature Why? CPU % <code>cpu_volatility</code> High volatility (jagged usage) is different from sustained load. Disk Bytes <code>io_pressure_index</code> Combines Read+Write speed with Queue Depth to measure bottlenecking. Time <code>cyclic_time_encoding</code> Converting time (0-24h) into Sin/Cos waves so the model understands 23:59 is close to 00:01."},{"location":"ai/ml/#the-training-cycle-active-learning","title":"\ud83d\udd04 The Training Cycle (Active Learning)","text":"<p>The project is currently in an Active Data Accumulation Phase.</p> <ol> <li>Edge Collection: The Desktop Agent collects high-resolution telemetry.</li> <li>Cloud Storage: Data is stored in Supabase.</li> <li>Offline Training: We batch-train PyTorch models on this historical data.</li> <li>Deployment: The trained model weights (<code>.onnx</code> files) are pushed back to the Desktop Agent to run locally.</li> </ol> <p>This cycle allows the system to get smarter over time, learning the specific \"personality\" of the user's hardware.</p>"},{"location":"ai/rag/","title":"RAG &amp; Vector Database","text":"<p>Retrieval-Augmented Generation (RAG) is the mechanism that gives the Generative AI \"textbooks\" to study from. Without RAG, an AI only knows what it learned during its initial training (which might be months old).</p> <p>With RAG, we can dynamically feed the AI up-to-date documentation, specific error code dictionaries, and platform-specific troubleshooting guides. This ensures that when the AI suggests a fix for an \"Arch Linux\" update error, it uses the correct <code>pacman</code> commands, not <code>apt-get</code>.</p>"},{"location":"ai/rag/#why-rag-is-essential","title":"\ud83d\udcda Why RAG is Essential","text":"<p>In a technical diagnostics system, accuracy is paramount.</p> <ul> <li>The Hallucination Problem: If you ask a standard LLM about a specific error code, it might guess.</li> <li>The Solution: RAG forces the LLM to look up the answer in a trusted \"Knowledge Base\" first, then answer using only that retrieved information.</li> </ul>"},{"location":"ai/rag/#architecture-pipeline","title":"\ud83c\udfd7 Architecture Pipeline","text":"<p>The RAG system operates in two distinct phases: Ingestion (Learning) and Retrieval (Answering).</p>"},{"location":"ai/rag/#phase-1-ingestion-building-the-knowledge-base","title":"Phase 1: Ingestion (Building the Knowledge Base)","text":"<p>We collect high-quality technical documentation and prepare it for the AI.</p> <ol> <li>Sourcing: We scrape reliable sources (Microsoft Learn, Arch Wiki, Ubuntu Manpages).</li> <li>Chunking: Large documents are split into smaller paragraphs (e.g., 500 characters).</li> <li>Embedding: An Embedding Model (like OpenAI <code>text-embedding-3-small</code>) converts these text chunks into Vectors (lists of floating-point numbers) that represent their meaning.</li> <li>Storage: These vectors are stored in Supabase using the <code>pgvector</code> extension.</li> </ol>"},{"location":"ai/rag/#phase-2-retrieval-the-search","title":"Phase 2: Retrieval (The Search)","text":"<p>When a user asks a question:</p> <ol> <li>Query Embedding: The user's question is converted into a vector.</li> <li>Semantic Search: We query the database for the \"closest\" vectors (using Cosine Similarity). This finds documents that mean the same thing, even if they use different keywords.</li> <li>Augmentation: The top 3 matching documents are pasted into the prompt sent to the LLM.</li> </ol>"},{"location":"ai/rag/#the-vector-store-supabase-pgvector","title":"\ud83d\udcbe The Vector Store (Supabase pgvector)","text":"<p>We utilize PostgreSQL with the <code>pgvector</code> extension as our Vector Database. This simplifies the architecture significantly.</p> <p>Instead of needing a separate database (like Pinecone or Weaviate), our vectors live right alongside our user data and telemetry logs. This allows us to perform \"Hybrid Searches\"\u2014filtering by both meaning and metadata (e.g., \"Search for 'Blue Screen' fixes, but only in the 'Windows 11' documentation\").</p>"},{"location":"ai/rag/#integration-with-the-agent","title":"\ud83d\udee0 Integration with the Agent","text":"<p>The RAG system functions as a Tool for the Agentic AI.</p> <ul> <li>Scenario: User asks \"What does error 0x80070005 mean?\"</li> <li>Action: The Agent realizes it doesn't know this specific code.</li> <li>Tool Use: It calls the <code>VectorSearch</code> tool.</li> <li>Result: The system retrieves the official Microsoft documentation for \"Access Denied\" errors.</li> <li>Response: The AI explains the error and suggests checking file permissions, citing the official source.</li> </ul>"},{"location":"ai/rag/#benefits-for-device-health","title":"\ud83c\udfaf Benefits for Device Health","text":"<ol> <li>OS Awareness: The system creates separate \"knowledge partitions\" for Windows, Ubuntu, and Arch, preventing command confusion.</li> <li>Updatability: If a new Windows update causes a new bug, we can simply add the new documentation to the database. The AI becomes \"smarter\" instantly, without needing to be retrained.</li> <li>Traceability: The AI can cite its sources (e.g., \"According to the Arch Wiki...\"), increasing user trust.</li> </ol>"},{"location":"backend/api/","title":"API &amp; Security Architecture","text":"<p>In a traditional web application, developers manually write API endpoints (e.g., creating a Python function for <code>/get-users</code>).</p> <p>This project adopts a Serverless / Backend-as-a-Service (BaaS) architecture using Supabase. Instead of a custom middleware server, Supabase automatically generates a high-performance REST API and WebSocket layer directly from our PostgreSQL database schema.</p>"},{"location":"backend/api/#the-auto-generated-api-postgrest","title":"\ud83d\udd0c The Auto-Generated API (PostgREST)","text":"<p>The core API layer is powered by PostgREST. This tool turns the database tables directly into RESTful endpoints.</p> <ul> <li>Database Table: <code>public.system_metrics</code></li> <li>Auto-Generated Endpoint: <code>https://[project-ref].supabase.co/rest/v1/system_metrics</code></li> </ul> <p>This means that as soon as we define a new table in the database, the API is instantly ready to accept data from the Desktop Agent or serve data to the Web Dashboard. No extra deployment code is required.</p>"},{"location":"backend/api/#supported-operations","title":"Supported Operations","text":"<p>The API supports standard HTTP verbs mapped to database actions: * GET: Read telemetry (Select). * POST: Upload new reports (Insert). * PATCH: Update user settings (Update). * DELETE: Clear old logs (Delete).</p>"},{"location":"backend/api/#real-time-api-websockets","title":"\u26a1 Real-Time API (WebSockets)","text":"<p>Beyond standard request/response cycles, the platform utilizes Real-Time Subscriptions.</p> <p>The Desktop Agent writes to the database, and Supabase broadcasts these changes via WebSockets to any connected Web Dashboard clients. This allows the dashboard to update graphs instantly without needing to refresh the page or poll the server every few seconds.</p> <p>Key Use Case:</p> <p>If the Desktop Agent detects a temperature spike (&gt;90\u00b0C), it writes a row to the <code>alerts</code> table. The Web Dashboard, subscribed to <code>alerts</code>, receives this event in milliseconds and triggers a visual warning.</p>"},{"location":"backend/api/#security-architecture","title":"\ud83d\udee1 Security Architecture","text":"<p>Since the API URL is public and exposed to the internet, security is not handled by hiding the API, but by authorizing the data.</p>"},{"location":"backend/api/#1-json-web-tokens-jwt","title":"1. JSON Web Tokens (JWT)","text":"<p>Every request made to the API must include a valid Bearer Token. * Authentication: When a user logs in via Google OAuth, they receive a JWT. * Transport: This token is sent in the header of every HTTP request (<code>Authorization: Bearer &lt;token&gt;</code>). * Verification: Supabase verifies the signature of the token to ensure it hasn't been forged.</p>"},{"location":"backend/api/#2-row-level-security-rls","title":"2. Row Level Security (RLS)","text":"<p>This is the firewall of our database. Even if a hacker has a valid API token, RLS restricts what they can touch.</p> <ul> <li>The Rule: <code>auth.uid() = user_id</code></li> <li>The Effect: A user can only run <code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, or <code>DELETE</code> on rows that contain their specific User ID.</li> </ul>"},{"location":"backend/api/#3-api-gateway-protection","title":"3. API Gateway Protection","text":"<p>Supabase places an API Gateway (Kong) in front of the database to handle: * Rate Limiting: Preventing a malfunctioning Desktop Agent from spamming the server with millions of requests. * DDoS Protection: Filtering out malicious traffic. * SSL/TLS: Enforcing encrypted connections (HTTPS) for all data in transit.</p>"},{"location":"backend/api/#service-role-access","title":"\ud83d\udd11 Service Role Access","text":"<p>While the Desktop App and Web Dashboard use restricted \"User Keys\" (public), the backend administrative scripts use a Service Role Key (private).</p> <ul> <li>Public Key (Anon): Safe to embed in the PyQt5 app and Streamlit code. Restricted by RLS.</li> <li>Service Key (Admin): Bypasses all RLS rules. Used only for background maintenance tasks (like training ML models on aggregate data) running in a secure, isolated server environment.</li> </ul>"},{"location":"backend/schema/","title":"Data Schema (PostgreSQL)","text":"<p>The application's data layer is built on Supabase (PostgreSQL). The schema relies heavily on JSONB columns to handle flexible, document-style data (like chat logs and complex system reports) while maintaining the querying power of SQL.</p>"},{"location":"backend/schema/#core-tables","title":"\ud83d\uddc2 Core Tables","text":"<p>Based on the current deployment, the database is structured into three primary tables:</p>"},{"location":"backend/schema/#1-telemetry-analysis-user_system_reports","title":"1. Telemetry &amp; Analysis (<code>user_system_reports</code>)","text":"<p>This is the central table where the Desktop Agent uploads data. It acts as both a Log of raw metrics and a Report containing AI analysis.</p> <ul> <li><code>id</code> (UUID): Unique identifier for the report.</li> <li><code>user_email</code> (text): Links the report to the user (via email identity).</li> <li><code>device_name</code> (text): The hostname of the machine (e.g., <code>Desktop-Main</code>).</li> <li><code>os</code> (text): Operating System details (e.g., <code>Windows 11</code>).</li> <li><code>raw_data</code> (jsonb): The complete, unprocessed telemetry payload from <code>psutil</code> (CPU, RAM, Disk I/O).</li> <li><code>summary</code> (json): AI-generated summary of the raw data.</li> <li><code>conclusions</code> (text): High-level diagnostic output (e.g., \"System is healthy\").</li> <li><code>status</code> (text): Current health tag (e.g., <code>Critical</code>, <code>Warning</code>, <code>Healthy</code>).</li> <li><code>source</code> (text): Origin of the data (e.g., <code>desktop-agent-v1</code>).</li> <li><code>created_at</code> (timestamptz): Upload timestamp.</li> <li><code>processed_at</code> (timestamptz): Timestamp when the AI finished analyzing the report.</li> </ul>"},{"location":"backend/schema/#2-chat-history-all_chats","title":"2. Chat History (<code>all_chats</code>)","text":"<p>Instead of storing messages as individual rows, we store entire conversation threads as JSON arrays. This simplifies retrieving the full context for the LLM.</p> <ul> <li><code>chat_id</code> (int8): Unique ID for the conversation thread.</li> <li><code>email</code> (text): The owner of the chat.</li> <li><code>title</code> (text): Auto-generated title of the conversation.</li> <li><code>user_messages</code> (json): Array of all prompts sent by the user.</li> <li><code>llm_responses</code> (json): Array of all responses generated by the AI.</li> <li><code>summary</code> (jsonb): Key takeaways from the conversation.</li> <li><code>metadata</code> (jsonb): Additional context tokens or model usage stats.</li> </ul>"},{"location":"backend/schema/#3-chat-sequence-tracking-user_chat_nums","title":"3. Chat Sequence Tracking (<code>user_chat_nums</code>)","text":"<p>A utility table used to manage and increment chat IDs for users, ensuring sequential or unique ordering.</p> <ul> <li><code>chat_id</code> (int8): The current max chat ID.</li> <li><code>email</code> (text): The user associated with this sequence.</li> </ul>"},{"location":"backend/schema/#vector-store-chromadb","title":"\ud83e\udde0 Vector Store (ChromaDB)","text":"<p>Unlike the relational data stored in Supabase, the Vector Embeddings used for RAG are stored in ChromaDB.</p> <ul> <li>Role: Stores embeddings of technical documentation (Windows/Linux error codes).</li> <li>Integration: The Python backend queries ChromaDB for context before sending a prompt to the LLM, but the actual vector data does not reside in the PostgreSQL <code>public</code> schema.</li> </ul>"},{"location":"backend/schema/#relationships","title":"\ud83d\udd17 Relationships","text":"<p>The data uses Email as the primary linking key rather than raw UUIDs in some places, allowing for easier human-readable queries during debugging.</p> <pre><code>erDiagram\n    User ||--o{ user_system_reports : \"uploads\"\n    User ||--o{ all_chats : \"participates_in\"\n    User ||--o{ user_chat_nums : \"tracks_sequence\"\n\n    user_system_reports {\n        uuid id PK\n        text user_email FK\n        jsonb raw_data\n        text status\n    }\n\n    all_chats {\n        int8 chat_id PK\n        text email FK\n        json user_messages\n        json llm_responses\n    }</code></pre>"},{"location":"backend/schema/#data-retention-policies","title":"\ud83d\udee1 Data Retention Policies","text":"<p>To prevent the user_system_reports table from growing infinitely, we implement a cleanup strategy via pg_cron or Edge Functions:</p> <p>High Resolution: Raw jsonb telemetry is kept for 7 days.</p> <p>Archival: After 7 days, older reports are deleted or summarized into lightweight stats, freeing up storage space.</p>"},{"location":"backend/supabase/","title":"Supabase Architecture","text":"<p>The backend infrastructure of System Health AI is built entirely on Supabase, an open-source Backend-as-a-Service (BaaS) platform.</p> <p>By choosing Supabase, we replace the complexity of managing separate servers (e.g., an AWS EC2 instance for the API, an RDS instance for the database, and a separate Auth0 service for login) with a single, unified ecosystem built on top of PostgreSQL.</p>"},{"location":"backend/supabase/#the-stack-components","title":"\ud83c\udfd7 The Stack Components","text":"<p>Supabase is not just a database; it is a suite of tools that wraps the database. We utilize five key components:</p>"},{"location":"backend/supabase/#1-postgresql-the-core","title":"1. PostgreSQL (The Core)","text":"<p>Everything in Supabase starts with the database. Unlike NoSQL solutions (like Firebase), Supabase offers a full relational SQL environment. * Role: Stores User Profiles, Device Metadata, and Telemetry. * Extensions: We enable <code>pgvector</code> for AI memory and <code>pg_cron</code> for automated data cleanup.</p>"},{"location":"backend/supabase/#2-gotrue-authentication","title":"2. GoTrue (Authentication)","text":"<p>This service manages users and issues JWTs (JSON Web Tokens). * Role: Handles the Google OAuth handshake for both the Desktop App and Web Dashboard. * Benefit: It unifies the identity. A user logged into the PyQt5 app has the exact same User ID (UUID) as the user on the Streamlit web app.</p>"},{"location":"backend/supabase/#3-postgrest-the-api-layer","title":"3. PostgREST (The API Layer)","text":"<p>As detailed in the API section, this service automatically turns our SQL tables into a secure REST API. * Role: Allows the Desktop Agent to upload JSON reports via standard HTTP POST requests without us writing a single line of backend API code.</p>"},{"location":"backend/supabase/#4-realtime-websockets","title":"4. Realtime (WebSockets)","text":"<p>This service listens to the PostgreSQL Write-Ahead Log (WAL). * Role: Pushes updates to the Web Dashboard instantly. * Scenario: If the Desktop Agent writes a \"Critical Thermal Warning\" to the database, the Realtime engine instantly pushes that event to the open Web Dashboard, triggering an alert UI.</p>"},{"location":"backend/supabase/#5-edge-functions-serverless-compute","title":"5. Edge Functions (Serverless Compute)","text":"<ul> <li>Role: Runs custom TypeScript code triggered by database events.</li> <li>Use Case: We use Edge Functions to sanitize data or trigger external webhooks when a specific anomaly threshold is breached.</li> </ul>"},{"location":"backend/supabase/#the-glue-architecture","title":"\ud83d\udd04 The \"Glue\" Architecture","text":"<p>Supabase acts as the bridge between the two distinct halves of our project:</p> <ol> <li> <p>The \"Writer\" (Desktop Agent):</p> <ul> <li>Connects via the REST API.</li> <li>Focuses on Ingestion (High-frequency writes).</li> <li>Authenticated via Device/User Tokens.</li> </ul> </li> <li> <p>The \"Reader\" (Web Dashboard):</p> <ul> <li>Connects via the Supabase Python SDK.</li> <li>Focuses on Querying and Analysis.</li> <li>Uses Vector Search for RAG operations.</li> </ul> </li> </ol>"},{"location":"backend/supabase/#why-supabase","title":"\ud83d\ude80 Why Supabase?","text":""},{"location":"backend/supabase/#vs-awsgoogle-cloud","title":"vs. AWS/Google Cloud","text":"<p>Setting up an equivalent stack on AWS would require configuring RDS, Lambda, API Gateway, Cognito, and Kinesis. Supabase provides all of this out-of-the-box, allowing us to focus on the AI/ML logic rather than DevOps.</p>"},{"location":"backend/supabase/#vs-firebase","title":"vs. Firebase","text":"<p>While Firebase is popular, it is a NoSQL document store. * SQL Power: System monitoring data is highly structured (time-series). SQL is much better suited for querying \"Average CPU usage over the last 7 days\" than a NoSQL document store. * Vector Support: Firebase does not natively support Vector Embeddings for AI. Supabase's <code>pgvector</code> makes it an all-in-one solution for our Generative AI needs.</p>"},{"location":"backend/supabase/#scalability","title":"\ud83c\udf0d Scalability","text":"<p>The architecture is designed to scale horizontally. * Connection Pooling: Supabase (via Supavisor) manages thousands of simultaneous connections from desktop agents without crashing the database. * Partitioning: As data grows, we can partition the <code>system_metrics</code> table by time (e.g., one partition per month) to keep queries fast.</p>"},{"location":"desktop/auth/","title":"Authentication (Google OAuth)","text":"<p>Authentication in the Desktop Agent is handled via Google OAuth 2.0, managed through the Supabase Auth infrastructure.</p> <p>To ensure maximum security and compliance with Google's security protocols, the application utilizes the System Browser Flow (Loopback Interface) rather than an embedded web view.</p>"},{"location":"desktop/auth/#the-authentication-strategy","title":"\ud83d\udd10 The Authentication Strategy","text":"<p>We purposefully avoid entering credentials directly into the PyQt5 application. Instead, we delegate authentication to the user's trusted default browser (Chrome, Edge, Firefox, etc.).</p> <p>Why this approach? 1.  Security: The application never sees the user's password. 2.  Trust: Users log in via the familiar Google SSL interface. 3.  Compliance: Google blocks OAuth requests from embedded webviews (like <code>QWebEngineView</code>) to prevent phishing attacks.</p>"},{"location":"desktop/auth/#the-login-workflow","title":"\ud83d\udd04 The Login Workflow","text":"<p>The authentication process follows a strict sequence:</p> <ol> <li>Initiation: The user clicks \"Login with Google\" in the PyQt5 app.</li> <li>Redirect: The app launches the system's default web browser with a specific Supabase OAuth URL.</li> <li>Consent: The user signs in with their Google account in the browser.</li> <li>Callback: Upon success, Google redirects the browser to a local callback server running on the user's machine (e.g., <code>http://localhost:3000/callback</code>).</li> <li>Token Exchange: The PyQt5 app listens for this callback, captures the Access Token and Refresh Token, and closes the browser tab.</li> <li>Persistence: Tokens are securely stored locally for future sessions.</li> </ol>"},{"location":"desktop/auth/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>sequenceDiagram\n    participant User\n    participant DesktopApp as PyQt5 Agent\n    participant Browser as System Browser\n    participant Supabase as Supabase Auth\n    participant Google\n\n    User-&gt;&gt;DesktopApp: Clicks \"Login\"\n    DesktopApp-&gt;&gt;Browser: Opens Auth URL\n    Browser-&gt;&gt;Google: Requests Access\n    Google-&gt;&gt;User: Asks for Credentials\n    User-&gt;&gt;Google: Enters Credentials\n    Google-&gt;&gt;Supabase: Verifies Credentials\n    Supabase-&gt;&gt;Browser: Redirects to localhost/?access_token=...\n    Browser-&gt;&gt;DesktopApp: Passes Token via Callback\n    DesktopApp-&gt;&gt;DesktopApp: Validates &amp; Stores Token\n    DesktopApp-&gt;&gt;User: Updates UI to \"Connected\"</code></pre>"},{"location":"desktop/auth/#technical-implementation","title":"\ud83d\udee0 Technical Implementation","text":"<p>The implementation relies on the Supabase Python SDK (gotrue-py) and a local socket listener.</p> <ol> <li> <p>Generating the Auth URL The application requests a provider URL from Supabase, specifying the scope to access basic profile information (email, name, avatar).</p> </li> <li> <p>The Local Callback Listener To capture the token returning from the browser, the desktop application spins up a temporary local TCP server.</p> </li> </ol> <p>Port: Randomly assigned or fixed (e.g., 3000).</p> <p>Behavior: Listens for a single HTTP GET request containing the authentication fragment, parses the URL parameters, and then shuts down immediately to free resources.</p> <ol> <li>Session Management Once authenticated, the access_token and refresh_token are stored securely.</li> </ol> <p>Access Token: Used to authenticate API requests (uploading metrics, fetching configs). Expires after 1 hour.</p> <p>Refresh Token: Used to silently acquire a new Access Token without asking the user to log in again.</p>"},{"location":"desktop/auth/#security-considerations","title":"\ud83d\udee1 Security Considerations","text":"<p>Token Storage: Tokens are never stored in plain text files. They are encrypted using OS-specific secure storage (e.g., Windows Credential Manager or GNOME Keyring) where possible.</p> <p>Scope Limitation: The app requests only the minimum required permissions (public profile), ensuring user privacy.</p> <p>State Verification: A unique state parameter is passed during the OAuth flow to prevent CSRF (Cross-Site Request Forgery) attacks.</p>"},{"location":"desktop/intro/","title":"Desktop System Agent","text":"<p>The Desktop System Agent is a standalone client-side application built with PyQt5. It serves as the primary data collection and local intelligence node for the System Health AI platform.</p> <p>Unlike web-based monitoring tools that are restricted by browser sandboxing, this native application has direct access to low-level system APIs, allowing it to gather high-fidelity telemetry and perform real-time hardware analysis.</p>"},{"location":"desktop/intro/#core-responsibilities","title":"\ud83c\udfaf Core Responsibilities","text":"<p>The Desktop Agent is designed to fulfill three critical roles:</p> <ol> <li> <p>Telemetry Collection    Using <code>psutil</code> and platform-specific drivers to harvest granular metrics (CPU cycles, thermal zones, memory pages) that browsers cannot access.</p> </li> <li> <p>Edge Intelligence (Local ML)    Running lightweight neural networks locally to detect anomalies in real-time. This reduces cloud latency and ensures that critical alerts are generated even if the internet connection is unstable.</p> </li> <li> <p>Secure Synchronization    Acting as a secure bridge between the user's hardware and the Supabase cloud backend, ensuring all data is authenticated via Google OAuth before upload.</p> </li> </ol>"},{"location":"desktop/intro/#architecture-tech-stack","title":"\ud83c\udfd7 Architecture &amp; Tech Stack","text":"Component Technology Reasoning GUI Framework PyQt5 Robust, cross-platform (Windows/Linux), and natively integrates with Python's data science stack. Metrics Engine <code>psutil</code> Industry standard for cross-platform hardware monitoring. Network Layer <code>requests</code> / Supabase Handles secure REST API calls and real-time database synchronization. Concurrency <code>QThread</code> Ensures the UI remains responsive while heavy data collection runs in the background."},{"location":"desktop/intro/#application-workflow","title":"\ud83d\udd04 Application Workflow","text":"<p>When the user launches the Desktop Agent, the following lifecycle executes:</p> <ol> <li>Initialization: The app checks for local configuration files and validates the internet connection.</li> <li>Authentication: A Google OAuth window opens. Upon success, a secure session token is cached.</li> <li>Monitoring Loop:<ul> <li>Fetch: <code>psutil</code> queries hardware sensors.</li> <li>Analyze: The local ML engine scores the data frame for anomalies.</li> <li>Visualize: Real-time graphs update on the desktop UI.</li> </ul> </li> <li>Reporting: Every N seconds (configurable), a structured JSON report is cryptographically signed and uploaded to the Supabase backend.</li> </ol>"},{"location":"desktop/intro/#user-interface-design","title":"\ud83d\udda5 User Interface Design","text":"<p>The application is designed to be unobtrusive.</p> <ul> <li>Main Dashboard: Displays a \"Health Score\" speedometer and live sparklines for CPU/RAM.</li> <li>Status Bar: Shows connectivity status to the Cloud Brain.</li> <li>System Tray: The app can minimize to the tray to run silently in the background, only notifying the user via toast notifications if an anomaly (e.g., thermal throttling) is detected.</li> </ul>"},{"location":"desktop/intro/#why-a-desktop-app","title":"\ud83d\ude80 Why a Desktop App?","text":"<p>Web browsers are sandboxed for security. They cannot read your CPU temperature or see exactly which process is eating your RAM.</p> <p>By running a native PyQt5 application, we bridge the gap between hardware visibility and cloud intelligence, enabling the Agentic AI to \"see\" exactly what is happening on your machine.</p>"},{"location":"desktop/metrics/","title":"System Metrics Engine (psutil)","text":"<p>The core telemetry engine of the Desktop Agent is built upon <code>psutil</code> (process and system utilities). This cross-platform library allows the application to retrieve information on running processes and system utilization (CPU, memory, disks, network, sensors) in a portable way.</p> <p>This engine is responsible for the continuous high-frequency logging required to train the project's Neural Network models.</p>"},{"location":"desktop/metrics/#collected-metrics","title":"\ud83d\udcca Collected Metrics","text":"<p>To build a comprehensive health profile of the device, the agent collects data across five key subsystems:</p>"},{"location":"desktop/metrics/#1-cpu-telemetry","title":"1. CPU Telemetry","text":"<p>We monitor not just usage, but frequency and context switches to detect \"silent\" load. * Utilization: Per-core and aggregate percentage. * Frequency: Current, Minimum, and Maximum (Mhz). * Stats: Context switches, interrupts, and syscalls.</p>"},{"location":"desktop/metrics/#2-memory-ram-swap","title":"2. Memory (RAM &amp; Swap)","text":"<p>Tracking memory pressure helps predict system freezes before they happen. * Virtual Memory: Total, Available, Used, Free. * Swap Memory: Used, Free, Percent (critical for detecting thrashing).</p>"},{"location":"desktop/metrics/#3-disk-subsystem","title":"3. Disk Subsystem","text":"<ul> <li>Usage: Partition usage (Total/Used/Free).</li> <li>I/O Counters: Read count, Write count, Read bytes, Write bytes (essential for detecting ransomware-like behavior).</li> </ul>"},{"location":"desktop/metrics/#4-network-traffic","title":"4. Network Traffic","text":"<ul> <li>Bytes: Sent/Received.</li> <li>Packets: Sent/Received.</li> <li>Errors: Drop counts (indicates hardware or driver issues).</li> </ul>"},{"location":"desktop/metrics/#5-hardware-sensors","title":"5. Hardware Sensors","text":"<ul> <li>Temperatures: CPU and GPU thermal zones (critical for thermal throttling detection).</li> <li>Battery: Percentage, status (charging/discharging), and time remaining.</li> </ul> <p>!!! warning \"Platform Differences\"     Hardware sensors behave differently across OS versions. On Windows, administrative privileges are often required to read specific thermal zones, whereas Linux exposes them more freely via <code>/sys/class/thermal</code>. The engine handles these discrepancies gracefully using <code>try/except</code> blocks.</p>"},{"location":"desktop/metrics/#implementation-details","title":"\ud83d\udcbb Implementation Details","text":"<p>The collection logic runs on a dedicated background <code>QThread</code> to prevent freezing the GUI.</p>"},{"location":"desktop/metrics/#sample-collection-code","title":"Sample Collection Code","text":"<pre><code>import psutil\nimport time\n\ndef collect_snapshot():\n    return {\n        \"timestamp\": time.time(),\n        \"cpu\": {\n            \"percent\": psutil.cpu_percent(interval=None),\n            \"freq\": psutil.cpu_freq().current if psutil.cpu_freq() else 0,\n            \"cores\": psutil.cpu_count(logical=False)\n        },\n        \"memory\": {\n            \"total\": psutil.virtual_memory().total,\n            \"available\": psutil.virtual_memory().available,\n            \"percent\": psutil.virtual_memory().percent\n        },\n        \"disk_io\": psutil.disk_io_counters()._asdict() if psutil.disk_io_counters() else {},\n        \"net_io\": psutil.net_io_counters()._asdict()\n    }\n</code></pre>"},{"location":"desktop/metrics/#data-payload-structure","title":"\ud83d\udce6 Data Payload Structure","text":"<p>To facilitate Machine Learning training, data is normalized into a strictly typed JSON structure before being sent to Supabase. This consistency is vital for the ML expansion phase.</p> <p>Example Payload sent to Cloud:</p> <pre><code>{\n  \"device_id\": \"hw-uuid-550e-8400\",\n  \"session_id\": \"sess-8892-xxy\",\n  \"timestamp\": \"2023-10-27T10:00:00Z\",\n  \"metrics\": {\n    \"cpu_load\": 12.5,\n    \"cpu_temp\": 45.0,\n    \"ram_usage_percent\": 64.2,\n    \"disk_read_mb\": 120.5,\n    \"disk_write_mb\": 45.2,\n    \"net_sent_mb\": 1.2,\n    \"net_recv_mb\": 5.8\n  },\n  \"flags\": {\n    \"is_throttling\": false,\n    \"on_battery\": true\n  }\n}\n</code></pre>"},{"location":"desktop/metrics/#role-in-machine-learning","title":"\ud83e\udde0 Role in Machine Learning","text":"<p>This module is not just for display; it is a Data Ingestion Pipeline.</p> <p>By running continuously, the app accumulates a time-series dataset of \"normal\" vs. \"abnormal\" system behavior. This historical data is currently being aggregated in Supabase to train:</p> <p>Anomaly Detection Autoencoders: To flag unusual spikes in resource usage.</p> <p>Predictive LSTM Networks: To forecast system crashes based on rising thermal and memory trends.</p>"},{"location":"desktop/ml/","title":"Local ML Inference","text":"<p>While the cloud backend handles heavy generative AI tasks (like the Chatbot and RAG), the Desktop Agent is responsible for Edge Intelligence.</p> <p>The \"Local ML Engine\" is a lightweight, low-latency module designed to analyze system metrics in real-time, directly on the user's hardware. This ensures that critical alerts (like thermal runaway or crypto-mining malware) are detected immediately, even without an internet connection.</p>"},{"location":"desktop/ml/#why-local-inference","title":"\ud83e\udde0 Why Local Inference?","text":"<p>Running Machine Learning models locally on the client provides three key advantages:</p> <ol> <li>Zero Latency: Decisions are made in milliseconds. There is no network round-trip time.</li> <li>Privacy: Sensitive raw telemetry does not need to leave the device for basic health checks.</li> <li>Offline Protection: The system continues to monitor and protect the device even when disconnected from the internet.</li> </ol>"},{"location":"desktop/ml/#the-ml-pipeline","title":"\u2699\ufe0f The ML Pipeline","text":"<p>The local pipeline operates on a continuous loop, synchronized with the <code>psutil</code> metrics collector.</p>"},{"location":"desktop/ml/#1-preprocessing-normalization","title":"1. Preprocessing &amp; Normalization","text":"<p>Raw data from <code>psutil</code> often varies in scale (e.g., CPU % is 0-100, while RAM bytes are in the billions). * Scaler: Inputs are standardized using a pre-fitted <code>StandardScaler</code> (Z-score normalization) or MinMax scaling to ensure the neural network receives balanced inputs. * Windowing: Data is not analyzed as single points but as sliding windows (e.g., the last 60 seconds of activity) to capture trends rather than noise.</p>"},{"location":"desktop/ml/#2-anomaly-detection-models","title":"2. Anomaly Detection Models","text":"<p>The agent utilizes lightweight unsupervised models to identify deviations from \"normal\" behavior.</p> <ul> <li>Isolation Forest / One-Class SVM: Used to detect outliers in multidimensional space (e.g., high CPU usage + low disk I/O + high network traffic = potential malware).</li> <li>Statistical Thresholding: Dynamic baselines are calculated for thermal sensors. If a temperature exceeds the moving average by $3\\sigma$ (3 standard deviations), it is flagged.</li> </ul>"},{"location":"desktop/ml/#3-inference-engine","title":"3. Inference Engine","text":"<p>The actual execution uses highly optimized libraries to minimize CPU overhead. * Runtime: <code>ONNX Runtime</code> or <code>Scikit-learn</code>. * Format: Models are trained in the cloud and exported to <code>.onnx</code> or <code>.pkl</code> format, which the desktop agent downloads during updates.</p>"},{"location":"desktop/ml/#continuous-learning-data-expansion","title":"\ud83d\udd04 Continuous Learning &amp; Data Expansion","text":"<p>A core objective of the current project phase is Neural Network Expansion.</p> <p>To build robust Deep Learning models (such as LSTMs for failure prediction), the system requires a massive, diverse dataset of \"healthy\" vs. \"unhealthy\" machine states.</p>"},{"location":"desktop/ml/#the-training-loop","title":"The Training Loop","text":"<ol> <li>Data Collection: The desktop app runs continuously, logging labeled telemetry data.</li> <li>Cloud Upload: Anonymized feature vectors are sent to Supabase.</li> <li>Centralized Training: The backend uses this aggregate data to retrain and refine the neural networks.</li> <li>Model Push: Improved model weights are versioned and pushed back to the Desktop Agent.</li> </ol> <p>Current Status: The system is currently in an active Data Accumulation Phase, gathering high-fidelity metrics to train the next generation of predictive Neural Networks.</p>"},{"location":"desktop/ml/#code-architecture","title":"\ud83d\udcbb Code Architecture","text":"<p>The ML Engine is decoupled from the UI to ensure the app stays responsive.</p> <pre><code>class LocalMLEngine:\n    def __init__(self, model_path):\n        self.model = load_model(model_path)\n        self.scaler = load_scaler(model_path)\n        self.history_window = deque(maxlen=60) # Store last 60 seconds\n\n    def analyze(self, raw_metrics):\n        # 1. Preprocess\n        vector = self._vectorize(raw_metrics)\n        self.history_window.append(vector)\n\n        # 2. Inference (if window full)\n        if len(self.history_window) == 60:\n            flattened_window = np.array(self.history_window).flatten()\n            anomaly_score = self.model.decision_function([flattened_window])\n\n            # 3. Decision\n            if anomaly_score &lt; -0.5:\n                return AnomalyType.CRITICAL\n\n        return AnomalyType.NORMAL\n</code></pre>"},{"location":"desktop/ml/#future-neural-networks","title":"\ud83d\udd2e Future: Neural Networks","text":"<p>As the dataset grows, the Local ML Engine will transition from statistical models to deep neural networks:</p> <p>Autoencoders: for superior unsupervised anomaly detection.</p> <p>RNNs/LSTMs: for time-series forecasting (e.g., \"Your CPU will overheat in 10 minutes\").</p>"},{"location":"desktop/reports/","title":"Report Generation","text":"<p>The final responsibility of the Desktop Agent is to transform continuous raw telemetry into structured System Health Reports.</p> <p>A \"Report\" in this ecosystem is a rich JSON Data Object that maps directly to the <code>user_system_reports</code> table in Supabase. These objects act as the standardized communication protocol between the Local Device and the Cloud Brain.</p>"},{"location":"desktop/reports/#the-report-lifecycle","title":"\ud83d\udcc4 The Report Lifecycle","text":"<p>The generation process follows a \"Session-Based\" approach:</p> <ol> <li>Session Start: When the app launches, a unique session context is established.</li> <li>Aggregation: Data is collected locally.</li> <li>Structuring: Raw metrics are packaged into the <code>raw_data</code> JSONB structure.</li> <li>Tagging: The agent assigns a preliminary <code>status</code> (e.g., \"Healthy\", \"Warning\") based on local thresholds.</li> <li>Dispatch: The object is securely uploaded to Supabase.</li> </ol>"},{"location":"desktop/reports/#report-data-structure","title":"\ud83e\udde9 Report Data Structure","text":"<p>The report payload is designed to match the Supabase Schema exactly. Note that fields like <code>summary</code> and <code>conclusions</code> are often sent as <code>null</code> initially, to be filled later by the Cloud AI.</p>"},{"location":"desktop/reports/#json-payload-schema","title":"JSON Payload Schema","text":"<pre><code>{\n  id: (uuid) \n  user_email:... \n  raw_data : { data: {recent_samples: {..}, aggregates: {..}}} \n      \"summary\": {\n        \"status\": \"CRITICAL\",\n        \"global_threat_index\": 60,\n        \"recent_analysis\": {\n          \"anomalies_detected\": {\n            \"cpu\": false,\n            \"net\": false,\n            \"disk\": false,\n            \"temp\": false\n          },\n          \"recent_sample_count\": 90\n        },\n        \"aggregate_analysis\": {\n        ... \n        },\n        \"forecast_projection\": [\n          {...}, \n          {..}, \n\n\n        ],\n        \"peak_active_period\": {\n          \"top_processes\": [\n            ... \n          ],\n          \"top_aggregate\": {\n            ... \n          }\n        }\n    }\n  conclusions: {...}\n  status: processed\n  source : desktop_app\n  device_name:archlinux-awwab\n  os: Linux\n  created_at:...\n  processed_at:....\n}\n</code></pre>"},{"location":"desktop/reports/#data-integrity","title":"\ud83d\udee1\ufe0f Data Integrity","text":"<p>To prevent data tampering (e.g., malware trying to hide its tracks by modifying the report before upload), the agent implements a basic integrity check.</p> <p>Before transmission, the critical data payload is hashed. On the server side, this hash can be verified.</p> <pre><code>import hashlib\nimport json\n\ndef sign_report(report_data):\n    \"\"\"Generates a SHA-256 signature for the report payload.\"\"\"\n    # We sign the raw_data block specifically as it contains the critical metrics\n    payload_string = json.dumps(report_data['raw_data'], sort_keys=True).encode()\n    return hashlib.sha256(payload_string).hexdigest()\n</code></pre>"},{"location":"desktop/reports/#upload-mechanism","title":"\u2601\ufe0f Upload Mechanism","text":"<p>The upload process is handled by a separate worker thread to ensure the UI never hangs.</p> <p>Endpoint: Supabase Database (Table: user_system_reports)</p> <p>Method: insert (New row for every report)</p> <p>Retry Logic: If the internet is down, reports are cached locally in a temporary SQLite database and bulk-uploaded once connectivity is restored.</p>"},{"location":"desktop/reports/#integration-with-web-dashboard","title":"\ud83d\udd17 Integration with Web Dashboard","text":"<p>Once the report lands in user_system_reports:</p> <p>Visualization: The Web App queries the raw_data column to render historical graphs.</p> <p>AI Analysis: The backend triggers an AI process that analyzes the raw_data, fills in the summary and conclusions columns, and updates the processed_at timestamp.</p> <p>Alerting: If the Desktop Agent set the status to \"Critical\", the dashboard immediately highlights the device.</p>"},{"location":"journey/iterations/","title":"Iterations &amp; Improvements","text":"<p>No software is perfect in version 1.0. This project underwent significant refactoring as we encountered real-world bottlenecks. Below are the key technical pivots that defined the current architecture.</p>"},{"location":"journey/iterations/#pivot-1-from-polling-to-event-driven","title":"\ud83d\udd04 Pivot 1: From Polling to Event-Driven","text":"<p>The Problem: Initially, the Desktop App used a <code>while True:</code> loop with <code>time.sleep(1)</code>. * Issue: This blocked the Main UI thread. If the network request to the database took 2 seconds, the entire window would freeze, becoming unresponsive to clicks.</p> <p>The Fix: We rewrote the core engine using PyQt5 <code>QThread</code> and Signals. * Now, the \"Worker Thread\" handles the heavy lifting (<code>psutil</code> + Network I/O). * It emits a <code>data_ready</code> signal. * The \"Main Thread\" catches the signal and updates the UI. * Result: A buttery smooth 60 FPS interface, even during heavy data uploads.</p>"},{"location":"journey/iterations/#pivot-2-from-local-sqlite-to-cloud-supabase","title":"\ud83d\udd04 Pivot 2: From Local SQLite to Cloud Supabase","text":"<p>The Problem: Version 1 stored data in a local <code>data.db</code> (SQLite) file. * Issue: Users could only see data if they were physically sitting at that specific computer. There was no \"remote monitoring\" capability.</p> <p>The Fix: We migrated to Supabase (PostgreSQL). * This introduced the complexity of network latency and offline handling. * We added a Local Buffer: If the internet cuts out, the app saves reports to a temporary local file and batch-uploads them when the connection returns.</p>"},{"location":"journey/iterations/#pivot-3-from-hardcoded-auth-to-oauth","title":"\ud83d\udd04 Pivot 3: From Hardcoded Auth to OAuth","text":"<p>The Problem: Early versions required users to manually copy-paste a \"Device Key\" from the website to the desktop app <code>config.json</code>. * Issue: It was user-unfriendly and insecure. If a key was stolen, anyone could flood the database.</p> <p>The Fix: We implemented Google OAuth 2.0 with PKCE. * This required building a local HTTP server in Python to capture the browser redirect. * Result: A seamless \"Sign in with Google\" button that securely identifies the user without manual key management.</p>"},{"location":"journey/iterations/#pivot-4-from-basic-chatbot-to-agentic-ai","title":"\ud83d\udd04 Pivot 4: From Basic Chatbot to Agentic AI","text":"<p>The Problem: The first chatbot integration was just a wrapper around GPT-3.5. * Issue: If asked \"Why is my PC slow?\", it would give generic advice like \"Check Task Manager.\" It couldn't see the actual data.</p> <p>The Fix: We moved to an Agentic RAG Architecture. * We gave the LLM \"Tools\" (Python functions) to query the database. * Result: Now, the AI proactively looks at the RAM usage before answering, providing specific advice based on the actual telemetry.</p>"},{"location":"journey/learnings/","title":"Project Learnings","text":"<p>Building a distributed, AI-powered system provided deep insights into full-stack development, system-level programming, and AI orchestration. This project was not just about connecting libraries; it was about managing complexity across different environments (Desktop, Web, and Cloud).</p>"},{"location":"journey/learnings/#technical-takeaways","title":"\ud83e\udde0 Technical Takeaways","text":""},{"location":"journey/learnings/#1-the-main-thread-rule-pyqt5","title":"1. The \"Main Thread\" Rule (PyQt5)","text":"<p>One of the earliest and hardest lessons was understanding the GUI Event Loop. * The Mistake: Running <code>psutil</code> queries or network requests directly inside the button click function. * The Consequence: The application window would \"freeze\" and become unresponsive while waiting for the server. * The Lesson: We learned to religiously use <code>QThread</code> and <code>pyqtSignal</code>. Keep the UI thread purely for rendering, and offload everything else to background workers.</p>"},{"location":"journey/learnings/#2-the-stateless-web-streamlit","title":"2. The Stateless Web (Streamlit)","text":"<p>Coming from desktop development, Streamlit's execution model was a paradigm shift. * The Challenge: Streamlit reruns the entire Python script from top to bottom every time a user interacts with a widget. * The Lesson: We mastered <code>st.session_state</code> to persist data (like chat history and auth tokens) across these reruns. Without this, the user would be logged out every time they clicked a button.</p>"},{"location":"journey/learnings/#3-context-windows-are-precious-ai","title":"3. Context Windows are Precious (AI)","text":"<p>We initially thought we could just \"feed the logs to the AI.\" * The Reality: LLMs have token limits, and API calls are priced by the token. Sending 24 hours of raw JSON logs is prohibitively expensive and confusing for the model. * The Lesson: Data Aggregation is key. We built pre-processing logic to summarize 1,000 data points into a concise \"Health Snapshot\" before the AI ever sees it.</p>"},{"location":"journey/learnings/#security-architecture","title":"\ud83d\udee1 Security &amp; Architecture","text":""},{"location":"journey/learnings/#1-row-level-security-rls-api-logic","title":"1. Row Level Security (RLS) &gt; API Logic","text":"<p>In early prototypes, we tried to filter data in the Python code (<code>if user.id == row.user_id</code>). * The Risk: It is too easy to make a mistake and leak data. * The Lesson: Pushing security down to the database layer (Supabase RLS) is safer. If the database engine enforces the rules, the frontend code can be simpler and less fragile.</p>"},{"location":"journey/learnings/#2-oauth-on-desktop-is-hard","title":"2. OAuth on Desktop is Hard","text":"<p>Implementing \"Log in with Google\" on a website is standard. Doing it in a standalone desktop app is complex. * The Lesson: We learned about the PKCE (Proof Key for Code Exchange) flow and how to spin up ephemeral local socket servers to capture redirect tokens securely.</p>"},{"location":"journey/learnings/#product-philosophy","title":"\ud83d\udcc8 Product Philosophy","text":""},{"location":"journey/learnings/#data-insight","title":"Data != Insight","text":"<p>The biggest realization was that users do not care about raw numbers. * Initial Thought: \"Let's show them a cool graph of CPU interrupt requests.\" * User Feedback: \"Is my computer okay?\" * The Pivot: We shifted focus from \"Monitoring\" (showing data) to \"Diagnostics\" (explaining data). This drove the decision to integrate the Agentic AI to translate the graphs into plain English.</p>"},{"location":"journey/timeline/","title":"Project Timeline","text":"<p>The development of System Health AI was not a straight line; it was an iterative evolution. What started as a simple curiosity\u2014\"How do I get my CPU usage using Python?\"\u2014expanded layer by layer into the full-stack agentic platform it is today.</p>"},{"location":"journey/timeline/#phase-1-the-script-era","title":"\ud83d\udcc5 Phase 1: The \"Script\" Era","text":"<p>Focus: Raw Data Extraction</p> <p>The project began as a standalone Python script running in a terminal. * Goal: Learn how to access low-level hardware counters without C++. * Implementation: A simple loop using <code>psutil</code> and <code>time.sleep(1)</code> that printed CPU and RAM stats to the console. * Limitation: It was ephemeral. Once you closed the terminal, the data was gone. There was no history, no visualization, and no persistence.</p>"},{"location":"journey/timeline/#phase-2-the-desktop-application","title":"\ud83d\udcc5 Phase 2: The Desktop Application","text":"<p>Focus: Visualization &amp; UX</p> <p>To make the tool usable, we needed a Graphical User Interface (GUI). * Tech Shift: Adopted PyQt5 to wrap the logic in a desktop window. * Feature: Added real-time graphing using <code>pyqtgraph</code>. * Challenge: The GUI would freeze during data collection. * Solution: Implemented <code>QThread</code> to separate the heavy <code>psutil</code> queries from the UI rendering loop, creating a smooth, responsive experience.</p>"},{"location":"journey/timeline/#phase-3-the-cloud-bridge","title":"\ud83d\udcc5 Phase 3: The Cloud Bridge","text":"<p>Focus: Connectivity &amp; Identity</p> <p>A monitoring tool is useless if you have to be at the computer to see it. We needed to send data off-device. * Backend: Integrated Supabase (PostgreSQL) to store logs. * Identity: Implemented Google OAuth 2.0. * The Hurdle: Getting OAuth to work in a desktop app was difficult. We had to build a local socket server (<code>localhost</code>) to catch the browser redirect and capture the secure tokens. * Result: The \"Connected\" status. The desktop app could now identify who was using it and securely upload encrypted JSON payloads to the cloud.</p>"},{"location":"journey/timeline/#phase-4-the-web-dashboard","title":"\ud83d\udcc5 Phase 4: The Web Dashboard","text":"<p>Focus: Analytics &amp; Accessibility</p> <p>With data flowing into the database, we needed a way to view it remotely. * Frontend: Built a Streamlit web application. * Feature: Created interactive time-series charts (Plotly) to visualize CPU load and Thermal spikes over 24-hour periods. * Impact: Users could now check their home PC's health from their smartphone while away.</p>"},{"location":"journey/timeline/#phase-5-generative-ai-integration","title":"\ud83d\udcc5 Phase 5: Generative AI Integration","text":"<p>Focus: Insight vs. Data</p> <p>We realized that users didn't want to read graphs; they wanted answers. * Integration: Connected an LLM (Large Language Model) to the dashboard. * Feature: Added a \"Chat\" interface. * Mechanism: We engineered a pipeline to feed the current system metrics into the LLM's system prompt. * Result: Instead of just showing \"CPU: 99%\", the app could now say: \"Your CPU is at 99%, which is unusual for this time of day.\"</p>"},{"location":"journey/timeline/#phase-6-agentic-ai-rag-current-state","title":"\ud83d\udcc5 Phase 6: Agentic AI &amp; RAG (Current State)","text":"<p>Focus: Autonomous Reasoning</p> <p>The current phase moves beyond simple observations to complex diagnostics. * RAG: Implemented Vector Search (Supabase <code>pgvector</code>) to let the AI read documentation for specific error codes (Windows vs. Linux). * Agentic Workflow: Transitioned from a simple \"Chatbot\" to an \"Agent\" that can use tools. The AI can now decide on its own to query historical data if it suspects a trend. * Neural Expansion: The desktop app was updated to collect high-frequency training data, preparing the ground for the custom Neural Networks that will power the next generation of anomaly detection.</p>"},{"location":"journey/timeline/#future-horizons","title":"\ud83d\ude80 Future Horizons","text":"<p>The journey continues. The next steps involve moving from Detection to Prediction (using LSTMs) and eventually Auto-Remediation (allowing the agent to fix problems for you).</p>"},{"location":"web/auth/","title":"Authentication (Web Dashboard)","text":"<p>Authentication in the Web Dashboard is the gateway that ensures users can only access their own system data. Like the Desktop Agent, the Web Dashboard utilizes Google OAuth 2.0 powered by Supabase, providing a unified Single Sign-On (SSO) experience across the entire platform.</p>"},{"location":"web/auth/#the-streamlit-auth-challenge","title":"\ud83d\udd10 The Streamlit Auth Challenge","text":"<p>Streamlit applications are stateless by default. Every time a user interacts with a widget (clicks a button, types text), the entire Python script reruns from top to bottom.</p> <p>This creates a unique challenge for authentication: How do we persist a user's login session across reruns without forcing them to sign in every time they click a button?</p>"},{"location":"web/auth/#the-solution-stsession_state-query-parameters","title":"The Solution: <code>st.session_state</code> + Query Parameters","text":"<p>We solve this by utilizing Supabase's implicit grant flow combined with Streamlit's Session State management.</p>"},{"location":"web/auth/#the-web-login-flow","title":"\ud83d\udd04 The Web Login Flow","text":"<ol> <li>Landing: Unauthenticated users are presented with a \"Login with Google\" button.</li> <li>Redirect: Clicking the button redirects the browser to the Supabase Auth URL.</li> <li>Provider Handshake: The user signs in with Google.</li> <li>Callback: Google redirects the user back to the Streamlit app URL, appending the authentication token in the URL fragment (e.g., <code>https://myapp.streamlit.app/#access_token=...</code>).</li> <li>Token Capture: On the reload, the Streamlit app detects the URL parameters, extracts the token, verifies it with Supabase, and stores the user object in <code>st.session_state</code>.</li> <li>URL Cleanup: The app clears the sensitive token from the URL bar to prevent leakage.</li> </ol>"},{"location":"web/auth/#implementation-logic","title":"Implementation Logic","text":"<pre><code>import streamlit as st\nfrom supabase import create_client\n\ndef init_auth():\n    # 1. Check if user is already logged in via Session State\n    if 'user' in st.session_state:\n        return True\n\n    # 2. Check for Token in URL (returning from Google)\n    query_params = st.query_params\n    if 'access_token' in query_params:\n        try:\n            # Exchange token for session\n            session = supabase.auth.get_session_from_url(query_params['access_token'])\n            st.session_state['user'] = session.user\n            # Clear URL params to hide token\n            st.query_params.clear()\n            return True\n        except Exception as e:\n            st.error(f\"Login failed: {e}\")\n            return False\n\n    # 3. Show Login Button\n    return False\n\ndef login_ui():\n    st.title(\"System Health AI\")\n    if st.button(\"Login with Google\"):\n        # Redirect to Supabase OAuth provider\n        auth_url = supabase.auth.get_url_for_provider('google')\n        st.link_button(\"Continue to Google\", auth_url)\n</code></pre>"},{"location":"web/auth/#authorization-data-security","title":"\ud83d\udee1 Authorization &amp; Data Security","text":"<p>Authentication (Who are you?) is only half the battle. Authorization (What can you see?) is handled at the database level.</p> <p>Row Level Security (RLS) We do not filter data using Python logic alone (e.g., SELECT * FROM reports WHERE user_id = current_user). This is insecure because a bug in the code could leak data.</p> <p>Instead, we rely on Postgres Row Level Security policies in Supabase:</p> <p>The web app sends the user's Access Token with every database request.</p> <p>Supabase confirms the user's identity (UUID).</p> <p>The Postgres database automatically filters the rows.</p> <p>Policy: Enable SELECT for users where auth.uid() == user_id</p> <p>This ensures that even if a malicious user tries to manipulate the API calls from the browser console, the database effectively acts as a blank slate for any data that doesn't belong to them.</p>"},{"location":"web/auth/#unified-identity","title":"\ud83d\udd17 Unified Identity","text":"<p>A critical feature of this architecture is the Shared User ID (UID).</p> <p>When you log in to the Desktop App, Supabase assigns you a specific UUID (e.g., abc-123).</p> <p>That UUID is attached to every psutil metric uploaded.</p> <p>When you log in to the Web App with the same Google account, you receive the same UUID (abc-123).</p> <p>This shared identifier is the \"glue\" that allows the web dashboard to instantly display the telemetry collected by your desktop agent without any complex pairing codes or manual setup.</p>"},{"location":"web/chatbot/","title":"AI Chatbot Interface","text":"<p>The AI Chatbot is the primary interface for the \"Agentic AI\" system. Unlike generic chatbots (like ChatGPT) that rely solely on pre-trained knowledge, this agent is context-aware. It possesses direct access to the user's specific hardware telemetry, allowing it to act as a personalized Systems Administrator.</p>"},{"location":"web/chatbot/#the-agentic-difference","title":"\ud83e\udd16 The \"Agentic\" Difference","text":"<p>Traditional chatbots answer questions based on general knowledge. Our Diagnostics Agent operates differently by utilizing a reasoning loop:</p> <ol> <li>Observe: The agent listens to the user's query (e.g., \"Why is my system lagging?\").</li> <li>Investigate: It autonomously decides to look up the latest system data from the database.</li> <li>Analyze: It compares current metrics against healthy baselines to detect anomalies (like high RAM usage).</li> <li>Respond: It provides an answer grounded in the specific facts it just discovered.</li> </ol>"},{"location":"web/chatbot/#architecture-the-rag-context-pipeline","title":"\ud83c\udfd7 Architecture: The RAG + Context Pipeline","text":"<p>The chatbot is built on a hybrid architecture combining Retrieval-Augmented Generation (RAG) and Dynamic Context Injection.</p>"},{"location":"web/chatbot/#1-dynamic-context-injection-short-term-memory","title":"1. Dynamic Context Injection (Short-Term Memory)","text":"<p>Before the AI generates a single word, the system \"injects\" a snapshot of the computer's current health into the conversation. The AI immediately knows the CPU model, current temperature, and memory load without the user having to explain it.</p>"},{"location":"web/chatbot/#2-rag-knowledge-base-long-term-memory","title":"2. RAG Knowledge Base (Long-Term Memory)","text":"<p>When the agent encounters specific error codes or symptoms, it queries a Vector Database containing: * OS-specific troubleshooting guides (Windows/Linux). * Hardware thermal limits documentation. * Common software conflict resolutions.</p>"},{"location":"web/chatbot/#user-interface-experience","title":"\ud83d\udcbb User Interface Experience","text":"<p>The interface is designed to feel like a standard messaging app.</p> <ul> <li>Session History: The chat history is saved, allowing the user to refresh the page without losing the context of the diagnosis.</li> <li>Streaming Responses: Answers appear incrementally (like a typewriter) to keep the interaction feeling responsive and alive.</li> </ul>"},{"location":"web/chatbot/#capabilities-tools","title":"\ud83d\udee0 Capabilities &amp; Tools","text":"<p>The Agent is equipped with specific \"Tools\"\u2014functions that allow it to interact with the system data:</p> <ul> <li>Fetch Current Metrics: Allows the AI to see the exact state of the hardware right now.</li> <li>Check Historical Trends: The AI can look back in time to see if a problem (like overheating) has been happening all day.</li> <li>Search Knowledge Base: Used to find specific solutions for complex error messages.</li> </ul>"},{"location":"web/chatbot/#example-conversation-flow","title":"\ud83e\udde9 Example Conversation Flow","text":"<p>User: \"My computer feels slow.\"</p> <p>Agent's Internal Process: The agent checks the latest report and sees that Disk I/O is running at 100% capacity, while CPU and RAM are normal. It also notices a background virus scan process is active.</p> <p>Agent Response: \"Your CPU and RAM usage are actually normal, but your Disk usage is extremely high right now. It looks like a background security scan is running, which is likely causing the slowdown. It should resolve once the scan finishes.\"</p>"},{"location":"web/frontend/","title":"Frontend &amp; UX Design","text":"<p>The user interface of the Web Dashboard is built with a focus on clarity, responsiveness, and immediate insight. Using Streamlit's reactive component system, the frontend transforms complex JSON telemetry into an intuitive visual story.</p> <p>The goal is to move beyond the \"spreadsheet look\" of traditional monitoring tools and create a modern, dashboard-style experience.</p>"},{"location":"web/frontend/#design-philosophy","title":"\ud83c\udfa8 Design Philosophy","text":"<p>The design follows three core principles:</p> <ol> <li>\"Glanceability\": A user should be able to understand their system's health status within 5 seconds of opening the page.</li> <li>Visual Hierarchy: Critical alerts (like overheating) are displayed prominently at the top, while granular details (like specific process IDs) are tucked away in expandable sections.</li> <li>Minimalism: We reduce cognitive load by hiding configuration options in the sidebar, leaving the main stage dedicated purely to data visualization.</li> </ol>"},{"location":"web/frontend/#visual-components","title":"\ud83d\udcca Visual Components","text":"<p>The application utilizes a rich set of interactive widgets to represent different types of data:</p>"},{"location":"web/frontend/#1-the-metric-row","title":"1. The \"Metric\" Row","text":"<p>At the very top of the dashboard, users see high-level Delta Metrics. These show current values compared to the previous hour. * Example: \"CPU Temp: 45\u00b0C (\u2193 2\u00b0C)\" \u2014 The arrow and color (Green/Red) immediately indicate if the trend is improving or worsening.</p>"},{"location":"web/frontend/#2-time-series-charts","title":"2. Time-Series Charts","text":"<p>We use interactive line charts to display historical performance. * Interactivity: Users can zoom, pan, and hover over specific data points to see exact values. * Correlation: CPU load and Temperature are often plotted on the same axis, allowing users to visually confirm that a spike in usage caused a spike in heat.</p>"},{"location":"web/frontend/#3-gauge-indicators","title":"3. Gauge Indicators","text":"<p>For limits that have a hard ceiling (like RAM or Battery), we use progress bars or gauge charts. This provides a clear visual representation of \"how much headroom is left\" before the system hits a bottleneck.</p>"},{"location":"web/frontend/#layout-navigation","title":"\ud83d\udcf1 Layout &amp; Navigation","text":"<p>The layout is divided into two distinct zones to maintain focus:</p>"},{"location":"web/frontend/#the-sidebar-control-zone","title":"The Sidebar (Control Zone)","text":"<p>The left-hand sidebar contains all input controls. This is where the user selects: * Which device to monitor (if they have multiple). * The time range for the analysis (e.g., \"Last 24 Hours\"). * Navigation between the \"Dashboard,\" \"Chatbot,\" and \"Settings\" pages.</p>"},{"location":"web/frontend/#the-main-stage-data-zone","title":"The Main Stage (Data Zone)","text":"<p>The central area is strictly for output. It adapts based on the sidebar selections. By separating controls from data, we ensure the charts are never obscured by menus.</p>"},{"location":"web/frontend/#theming-responsiveness","title":"\ud83c\udf17 Theming &amp; Responsiveness","text":""},{"location":"web/frontend/#dark-mode-light-mode","title":"Dark Mode &amp; Light Mode","text":"<p>The interface automatically detects the user's system preference. * Dark Mode: The default for most technical users, reducing eye strain during late-night debugging sessions. * Light Mode: High-contrast mode for well-lit environments.</p>"},{"location":"web/frontend/#mobile-adaptability","title":"Mobile Adaptability","text":"<p>Since Streamlit designs are responsive by default, the dashboard automatically restacks components for mobile screens. A user viewing the dashboard on a smartphone sees a simplified, vertical layout, ensuring they can check their server's health while on the go without needing a separate mobile app.</p>"},{"location":"web/intro/","title":"Web Dashboard (Streamlit)","text":"<p>The Web Dashboard is the cloud-native command center of the System Health AI platform. Built using Streamlit, it acts as the centralized interface where users can visualize historical telemetry, interact with the AI diagnostics agent, and manage their account settings.</p> <p>While the Desktop Agent focuses on collection and immediate detection, the Web Dashboard focuses on long-term analysis, explanation, and interaction.</p>"},{"location":"web/intro/#core-responsibilities","title":"\ud83c\udfaf Core Responsibilities","text":"<p>The Web Application serves three distinct purposes in the architecture:</p> <ol> <li> <p>Data Visualization &amp; Analytics     It retrieves the structured reports uploaded by the Desktop Agent from Supabase and renders them into interactive, time-series visualizations. This allows users to see trends over days, weeks, or months\u2014something the lightweight desktop agent is not designed to do.</p> </li> <li> <p>The AI Command Hub     It hosts the Generative AI and Agentic components. Because Large Language Models (LLMs) and RAG pipelines require significant compute resources (or API calls), they reside here in the cloud application rather than burdening the user's local machine.</p> </li> <li> <p>Cross-Device Accessibility     Being a web app, it allows a user to check the health status of their home PC from a phone, laptop, or any other device with a browser.</p> </li> </ol>"},{"location":"web/intro/#tech-stack-reasoning","title":"\ud83c\udfd7 Tech Stack &amp; Reasoning","text":"Component Technology Reasoning Framework Streamlit Python-native framework ideal for data science apps. Allows rapid integration of ML models and plotting libraries (Altair/Plotly) without complex frontend boilerplate. Backend Supabase Acts as the \"Backend-as-a-Service,\" providing the database, auth, and vector store that Streamlit connects to. Auth Google OAuth Unified single-sign-on (SSO) ensuring the web user matches the desktop user. AI Engine LangChain / LlamaIndex Orchestrates the RAG pipeline and agentic workflows within the Streamlit session."},{"location":"web/intro/#dashboard-structure","title":"\ud83d\udda5 Dashboard Structure","text":"<p>The application is organized into several key pages:</p> <ul> <li>Home / Overview: A high-level status board showing the most recent health scores from all connected devices.</li> <li>Analytics Deep Dive: Detailed interactive charts for CPU, Memory, and Thermal history. Users can filter by date ranges to correlate system crashes with specific metrics.</li> <li>AI Diagnostics (Chat): The conversational interface where users ask, \"Why did my PC crash yesterday?\" and receive answers grounded in their actual system logs.</li> <li>Settings: Management of data retention policies and account details.</li> </ul>"},{"location":"web/intro/#data-flow-from-sensor-to-screen","title":"\ud83d\udd04 Data Flow: From Sensor to Screen","text":"<p>Unlike the Desktop App, the Web Dashboard does not collect live metrics from the machine it is running on. Instead, it acts as a Viewer for the Supabase database.</p> <pre><code>graph LR\n    subgraph Data_Source\n        DB[(Supabase DB)]\n    end\n\n    subgraph Streamlit_App\n        Query[SQL Query]\n        Process[Pandas DataFrame]\n        Viz[Plotly Charts]\n        AI[AI Analysis]\n    end\n\n    User((User))\n\n    DB --&gt;|Fetch JSON Reports| Query\n    Query --&gt;|Clean &amp; Normalize| Process\n    Process --&gt;|Render| Viz\n    Process --&gt;|Context Context| AI\n    Viz --&gt;|View| User\n    AI --&gt;|Chat| User</code></pre>"},{"location":"web/intro/#why-streamlit","title":"\ud83d\ude80 Why Streamlit?","text":"<p>Traditional web frameworks (React, Vue) require separate backend APIs (FastAPI/Flask) to serve data. Streamlit allows us to build a full-stack data application entirely in Python.</p> <p>This reduces development complexity and ensures that the Machine Learning engineers building the predictive models can also build the User Interface without context switching between languages.</p>"},{"location":"web/supabase/","title":"Supabase Backend (CRUD)","text":"<p>The Web Dashboard interacts with Supabase as its primary data source. While the Desktop Agent is responsible for writing telemetry to the cloud, the Web Dashboard is primarily responsible for reading and visualizing that data.</p> <p>Supabase acts as the \"Source of Truth\" for the entire platform, utilizing a PostgreSQL database to store structured JSON reports, user profiles, and chat history.</p>"},{"location":"web/supabase/#the-data-bridge","title":"\ud83c\udf09 The Data Bridge","text":"<p>The dashboard serves as a graphical interface for the underlying database. It performs standard CRUD (Create, Read, Update, Delete) operations to manage the user's session.</p>"},{"location":"web/supabase/#read-operations-data-visualization","title":"Read Operations (Data Visualization)","text":"<p>When a user opens the analytics tab, the dashboard queries the database for historical records. To keep the application fast, it does not fetch all data at once. Instead, it filters data based on:</p> <ol> <li>Time Range: Only fetching metrics for the specific window selected by the user (e.g., \"Last Hour\" or \"Last 24 Hours\").</li> <li>Device ID: Ensuring metrics are only pulled for the specific machine currently being viewed.</li> </ol>"},{"location":"web/supabase/#create-operations-chat-history","title":"Create Operations (Chat History)","text":"<p>Every interaction with the AI Chatbot is persisted. When the user asks a question, both the user's prompt and the AI's response are written to a <code>chat_logs</code> table. This allows the AI to remember the context of the conversation if the user refreshes the page.</p>"},{"location":"web/supabase/#real-time-capabilities","title":"\u26a1 Real-Time Capabilities","text":"<p>A key advantage of using Supabase is its real-time functionality.</p> <p>The dashboard subscribes to database changes. If the Desktop Agent uploads a new \"Critical Alert\" (e.g., high temperature) while the user is viewing the dashboard, the graph updates automatically without requiring a manual page refresh. This provides near-instant visibility into the system's current state.</p>"},{"location":"web/supabase/#security-via-row-level-security-rls","title":"\ud83d\udd12 Security via Row Level Security (RLS)","text":"<p>The web application relies on the database to enforce privacy, rather than filtering data in the frontend code.</p> <p>How it works: * Every request sent from the web dashboard includes the user's unique Authentication Token. * The database checks this token against the data being requested. * If the user tries to access a system report that belongs to a different User ID, the database returns an empty result set.</p> <p>This ensures that even if there is a bug in the web application's code, one user can never accidentally see another user's private system data.</p>"}]}